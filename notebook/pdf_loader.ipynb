{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e876bbc4",
   "metadata": {},
   "source": [
    "RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa1376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nevintoms/Desktop/Projects/RAG_Langchain/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b34a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: objectdetection.pdf\n",
      "  ✓ Loaded 21 pages\n",
      "\n",
      "Processing: embeddings.pdf\n",
      "  ✓ Loaded 27 pages\n",
      "\n",
      "Processing: attention.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Total documents loaded: 63\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1792fb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6923788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:1100]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880406a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 63 documents into 356 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\n",
      "Object Detection with Deep Learning: A Review\n",
      "Zhong-Qiu Zhao, Member, IEEE, Peng Zheng,\n",
      "Shou-tao Xu, and Xindong Wu, Fellow, IEEE\n",
      "Abstract—Due to object detection’s close relationship with\n",
      "video analysis and image understanding, it has attracted much\n",
      "research attention in recent years. Traditional object detection\n",
      "methods are built on handcrafted features and shallow trainable\n",
      "architectures. Their performance easily stagnates by constructing\n",
      "complex ensembles which combine multiple low-level image\n",
      "features with high-level context from object detectors and scene\n",
      "classiﬁers. With the rapid development in deep learning, more\n",
      "powerful tools, which are able to learn semantic, high-level,\n",
      "deeper features, are introduced to address the problems existing\n",
      "in traditional architectures. These models behave differently\n",
      "in network architecture, training strategy and optimization...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='are also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='valuable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='or sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='allows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiﬁcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='capacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiﬁcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a ﬁxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]–[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection, but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overﬁtting of training, lack of large scale\\ntraining data, limited computation power and insigniﬁcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n•The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n•Fast development of high performance parallel computing\\nsystems, such as GPU clusters;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='as ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n•Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n•Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverﬁtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efﬁcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='What prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‘dropout’ regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‘pixel’ can be viewed as a speciﬁc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='linear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='loss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiﬁcation layer. The conv feature maps are produced by\\nconvoluting 3*3 ﬁlter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n•Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n• The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiﬁcation and bounding box regression\\ninto a multi-task leaning manner).\\n• Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image super-resolution\\nreconstruction [54], [55], image classiﬁcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='age retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conﬁdences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at ﬁrst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiﬁcation problem, adopting a uniﬁed\\nframework to achieve ﬁnal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-\\nCNN with a SPP layer). The regression /classiﬁcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario ﬁrstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='works [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the ﬂowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a ﬁxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the ﬁnal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiﬁcation and localization. With pre-trained category-\\nspeciﬁc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classiﬁcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁ-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\n˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='use of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='recognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classiﬁcation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun’s CNN (e.g., max(x, 0) rectifying non-linearities and\\n“dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Cun’s CNN (e.g., max(x, 0) rectifying non-linearities and\\n“dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classiﬁcation and object detection.\\nThis paper is the ﬁrst to show that a CNN can lead to dra-\\n1\\nFig. 3. The ﬂowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiﬁes each region with class-speciﬁc linear SVMs.\\nbounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN ﬁrstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciﬁc ﬁne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n•Due to the existence of FC layers, the CNN requires a\\nﬁxed-size (e.g., 227×227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n•Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region, taking a great deal of time in the testing period.\\n•Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is\\nﬁne-tuned. Then the softmax classiﬁer learned by ﬁne-\\ntuning is replaced by SVMs to ﬁt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n• Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n•Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reﬁne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciﬁc CNN classiﬁers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiﬁcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a ﬁxed-size input. That’s\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]† 56.07 74.41 ±1.0\\nLLC [18]† 57.66 76.95 ±0.4\\nFK [19]† 61.69 77.78 ±0.6\\nDeCAF [13] - 86.91 ±0.7\\nZeiler & Fergus [4] 75.90‡ 86.5±0.5\\nOquab et al. [34] 77.7 -\\nChatﬁeld et al. [6] 82.42 88.54±0.3\\nours 82.44 93.42 ±0.5\\nTable 8: Classiﬁcation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). †numbers reported\\nby [27]. ‡our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brieﬂy review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN ﬁrst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='method [7]. R-CNN ﬁrst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a ﬁxed size (227 ×227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classiﬁer is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\ndeﬁne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the “fast” mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 ×1, 2×2, 3×3, 6×6, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 ×50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='d (256 ×50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classiﬁer for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classiﬁer\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='non-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns ∈ S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically ﬁnd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s ∈ S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 ×224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, ﬁne-tuning a network with log loss, training SVMs,\\nand ﬁnally ﬁtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the ﬁne-tuning al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tures, ﬁne-tuning a network with log loss, training SVMs,\\nand ﬁnally ﬁtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the ﬁne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (ﬁxed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that ﬁxes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it’s comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network ﬁrst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a ﬁxed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that ﬁnally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all “background” class and\\nanother layer that outputs four real-valued numbers for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ers: one that produces softmax probability estimates over\\nK object classes plus a catch-all “background” class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes reﬁned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a ﬁxed spatial extent of H × W (e.g., 7 × 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\ndeﬁned by a four-tuple (r, c, h, w ) that speciﬁes its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a ﬁxed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h × w RoI win-\\ndow into an H × W grid of sub-windows of approximate\\nsize h/H × w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets ['),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with ﬁve max pooling layers and between ﬁve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is conﬁgured by setting H and W to be\\ncompatible with the net’s ﬁrst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network’s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiﬁ-\\ncation) are replaced with the two sibling layers described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='H = W = 7 for VGG16).\\nSecond, the network’s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiﬁ-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speciﬁc bounding-box regressors).\\nThird, the network is modiﬁed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let’s elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inefﬁcient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inefﬁciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN and SPPnet networks are trained. The inefﬁciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe ﬁnal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 ×(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='also improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network ﬁne-tuning, SVM training and bounding-\\nbox regressor ﬁtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiﬁcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nﬁnally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one ‘background’\\nclass) and the other output layer encodes reﬁned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train\\nclassiﬁcation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +λ[u≥1]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =−log pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,··· ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u≥1] is employed to omit all background RoIs. To provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u≥1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to ﬁt\\nbounding-box regressors as below\\nLloc(tu,v) =\\n∑\\ni∈x,y,w,h\\nsmoothL1 (tu\\ni −vi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|−0.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefﬁcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at ﬁrst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='where R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efﬁciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciﬁc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers—a box-regression layer ( reg)\\nand a box-classiﬁcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nﬁeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and a box-classiﬁcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nﬁeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n×nconv layer followed by two sibling 1 ×1 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n×nconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW×H(typically ∼2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='translation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) ×800-dimensional output layer,\\nwhereas our method requires a (4+2)×9-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of overﬁtting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these deﬁnitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is deﬁned as:\\nL({pi},{ti}) = 1\\nNcls\\n∑\\ni\\nLcls (pi,p∗\\ni ) +λ 1\\nNreg\\n∑\\ni\\np∗\\ni Lreg(ti,t∗\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn×n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiﬁcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n×n conv layer followed by two sibling 1 ×1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n×n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n∑\\ni\\nLcls(pi,p∗'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n∑\\ni\\nLcls(pi,p∗\\ni) +λ 1\\nNreg\\n∑\\ni\\np∗\\niLreg(ti,t∗\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p∗\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t∗\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='With the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.\\nRecent state-of-the-art image classiﬁcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it’s\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll´ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='intensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signiﬁcant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object’s scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this ﬁgure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In this ﬁgure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k×k ﬁrstly and a position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k×k ﬁrstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiﬁcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='set of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 ×1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3×3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the ﬁnal feature map is\\ngenerated. This process is iterated until the ﬁnest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiﬁcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m×m mask to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m×m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='It affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='other tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a ﬂexible and efﬁcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiﬁcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‘stuff’ (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive ﬁelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiﬁers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='support regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiﬁcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiﬁed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='summing different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiﬁers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiﬁer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Traditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiﬁcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nﬁrstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /Classiﬁcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='eral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classiﬁcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We ﬁrstly reviews some pioneer CNN\\nmodels, and then focus on two signiﬁcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed\\nloss was introduced to bias both localization and conﬁdences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the ﬁnal layer.\\nYoo et al. adopted an iterative classiﬁcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\nﬁnding a path from a ﬁxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a ﬁxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifﬁculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conﬁdences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S×S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOUtruth\\npred , which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) ≥0) and\\nshows conﬁdences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ∗IOUtruth\\npred ∗Pr(Classi|Object)\\n= Pr(Classi) ∗IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='= Pr(Classi) ∗IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nλcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[\\n(xi −ˆxi)2 + (yi −ˆyi)2]\\n+λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[(√wi −\\n√\\nˆwi)2 + (\\n√\\nhi −\\n√\\nˆhi\\n)2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n(\\nCi −ˆCi\\n)2\\n+λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 noobj\\nij\\n(\\nCi −ˆCi\\n)2\\n+\\nS2\\n∑\\ni=0\\n1 obj\\ni\\n∑\\nc∈classes\\n(pi(c) −ˆpi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents conﬁdence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,\\nwhen the predictor is ‘responsible’ for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 ×1 reduction layers followed by 3 ×3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliﬁed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='makes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconﬁgurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciﬁc feature map, instead of\\nﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conﬁdences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conﬁdence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reﬁned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Integrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300×300) runs\\nat 59 FPS, which is more accurate and efﬁcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection results are obtained by conducting NMS on multi-scale reﬁned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciﬁc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiﬁcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n•If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n• With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n•Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n•Apart from basic models, there are still many other factors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='important for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n•Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nﬁed classiﬁcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n•As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and ﬁne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n•By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='•By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n• Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classﬁcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classﬁcation based approaches.\\n• Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n•Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='objects and surroundings (GBD-Net and multi-path).\\n•Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n•The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiﬁers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classiﬁcation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classiﬁcation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='FPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conﬁdence+background conﬁdence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conﬁdence+background conﬁdence\\n* ‘+’ denotes that corresponding techniques are employed while ‘-’ denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* ‘07’: VOC2007 trainval, ‘07+12’: union of VOC2007 and VOC2012 trainval, ‘07+12+COCO’: trained on COCO trainval35k at ﬁrst and then ﬁne-tuned on 07+12. The S in ION ‘07+12+S’ denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='HyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* ‘07++12’: union of VOC2007 trainval and test and VOC2012 trainval. ‘07++12+COCO’: trained on COCO trainval35k at ﬁrst then ﬁne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‘SS’ which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n• By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniﬁed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It’s also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='PV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n•It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n•It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n•Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiﬁed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniﬁed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial. To learn internal represen-\\ntations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive ﬁelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='convolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye ﬁxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='this problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Two standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as below\\nFβ = (1 +β2)Presion ×Recall\\nβ2Presion + Recall (7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH×W\\nH∑\\ni=1\\nW∑\\nj=1\\n⏐⏐⏐ˆS(i,j) = ˆZ(i,j)\\n⏐⏐⏐ (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can ﬁnd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufﬁcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='segmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it’s necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='complementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]–[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conﬁgurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='been proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='into smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefﬁcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wFβ 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wFβ 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wFβ 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wFβ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SOD wFβ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wFβ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiﬁcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniﬁed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a ﬁxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='addressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conﬁguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-ﬁne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='To reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reﬁne the positions of possible\\nfaces. Qin et al. proposed a uniﬁed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='types of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reﬂection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='NPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiﬁcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='which has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‘plain’ features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand ﬁne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='al. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiﬁers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and\\nsimpliﬁcation is of signiﬁcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‘Person (clear identiﬁcations)’,\\n‘Person? (unclear identiﬁcations)’ and ‘People (large group of\\nindividuals)’, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10−2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The ﬁrst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='candidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciﬁc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='application, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n•Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneﬁcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='jects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n•Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='selection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n•Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiﬁers at latter stages can\\nhandle more difﬁcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are ﬁxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='when training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n• Unsupervised and weakly supervised learning. It’s\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reﬁne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n•3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='transportation and intelligent surveillance.\\n•3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n• Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='blur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, “Pedestrian detection:\\nAn evaluation of the state of the art,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis.” IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235–245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\\nfast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='fast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, “Realtime multi-person\\n2d pose estimation using part afﬁnity ﬁelds,” in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, “Deepdriving:\\nLearning affordance for direct perception in autonomous driving,” in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object\\ndetection network for autonomous driving,” in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, “Embedded streaming\\ndeep neural networks accelerator with applications,” IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Neural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity\\napproximate convolutional neural networks,”IEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1–12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n“Cost-sensitive learning of deep feature representations from imbal-\\nanced data.” IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1–15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, “Feature extraction with deep\\nneural networks by a generalized discriminant analysis.” IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596–608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic segmentation,”\\nin CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\\nonce: Uniﬁed, real-time object detection,” in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\\ntime object detection with region proposal networks,” in NIPS, 2015,\\npp. 91–99.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\\ndetection,” in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, “An extended set of haar-like features for\\nrapid object detection,” in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,”Machine Learning,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y . Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[23] Y . Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627–1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, “The pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),” 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol.\\n521, no. 7553, pp. 436–444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, “Predicting eye ﬁxations\\nusing convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, “Face detection with the faster r-cnn,”\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, “Joint cascade face\\ndetection and alignment,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, “Supervised transformer network\\nfor efﬁcient face detection,” in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, “A real-time\\npedestrian detector using deep learning for human-aware navigation,”\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, “Exploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiﬁers,” in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, “A survey of deep learning methods and\\nsoftware tools for image classiﬁcation and object detection,” Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,”The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,”The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal\\nrepresentation by back-propagation of errors,” Nature, vol. 323, no.\\n323, pp. 533–536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality\\nof data with neural networks,” Sci., vol. 313, pp. 504–507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., “Deep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,” IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82–97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, “Binary coding of speech spectrograms using a deep auto-\\nencoder,” in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., “Phone recognition with\\nthe mean-covariance restricted boltzmann machine,” in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, “Improving neural networks by preventing co-\\nadaptation of feature detectors,” arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks\\nfor large-scale image recognition,” arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nboltzmann machines,” in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , “Weakly supervised\\nobject recognition with convolutional neural networks,” in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.\\n[51] F. M. Wadley, “Probit analysis: a statistical treatment of the sigmoid\\nresponse curve,” Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549–553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , “Learning invariant\\nfeatures through topographic ﬁlter maps,” in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, “Learning convolutional feature hierarchies for visual\\nrecognition,” in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Deconvolu-\\ntional networks,” in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for\\nsemantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive\\nsample learning,” in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, “Neural codes\\nfor image retrieval,” in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n“Deep learning for content-based image retrieval: A comprehensive\\nstudy,” in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. Barofﬁo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, “Deep convolutional neural networks for pedestrian detec-\\ntion,” Signal Process.: Image Commun. , vol. 47, pp. 482–489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, “Subcategory-aware\\nconvolutional neural networks for object proposals and detection,” in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='WACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n“Multimodal deep learning,” in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, “Modeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiﬁ-\\ncation,” in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904–1916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., “R-fcn: Object detection via region-based\\nfully convolutional networks,” in NIPS, 2016, pp. 379–387.\\n[66] T.-Y . Lin, P. Doll ´ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Belongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object\\ndetection using deep neural networks,” in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, “Attentionnet:\\nAggregating weak directions for accurate object detection,” in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, “G-cnn: an iterative grid\\nbased object detector,” in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, “Ssd: Single shot multibox detector,” in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,”\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd:\\nDeconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, “Dsod:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, “Dsod:\\nLearning deeply supervised object detectors from scratch,” in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-\\nencoders,” in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, “Learning invariance\\nthrough imitation,” in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n“Selective search for object recognition,” Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154–171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, “Pedestrian\\ndetection with unsupervised multi-stage feature learning,” in CVPR,\\n2013.\\n[80] P. Kr ¨ahenb¨uhl and V . Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2013.\\n[80] P. Kr ¨ahenb¨uhl and V . Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n“Multiscale combinatorial grouping,” in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll ´ar, “Edge boxes: Locating object proposals\\nfrom edges,” in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, “Deepbox: Learning objectness\\nwith convolutional networks,” in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ´ar, “Learning to\\nreﬁne object segments,” in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,” in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel ´aez, and J. Malik, “Learning rich features\\nfrom rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., “Deepid-net: Deformable deep convolutional\\nneural networks for object detection,” in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, “R-cnn minus r,” arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,”\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S ´anchez, and T. Mensink, “Improving the ﬁsher kernel\\nfor large-scale image classiﬁcation,” in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, “Restructuring of deep neural network\\nacoustic models with singular value decomposition.” in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\\nobject detection with region proposal networks,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\\ning the inception architecture for computer vision,” in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,” in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\\nmulti-task network cascades,” in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, “Fully convolutional instance-\\naware semantic segmentation,” in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial transformer networks,” in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, “Stuffnet: Using stuffto\\nimprove object detection,” in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, “Hypernet: Towards accurate\\nregion proposal generation and joint object detection,” in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, “Curriculum learning\\nof multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, “Multi-stage object\\ndetection with group recursive learning,” arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A uniﬁed multi-scale\\ndeep convolutional neural network for fast object detection,” in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,” in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling\\nwith lstm recurrent neural networks,” in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, “Learning to detect and\\nlocalize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-\\ndirectional cnn for object detection,” in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, “Object detection via a multi-region and\\nsemantic segmentation-aware cnn model,” in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-\\nworks,” IEEE Trans. Signal Process. , vol. 45, pp. 2673–2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll ´ar, “A multipath network for object detection,”\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\\nobject detectors with online hard example mining,” in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\\ndeep model for object detection with long-tail distribution,” in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, “Pvanet:\\nLightweight deep neural networks for real-time object detection,”\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and\\nimproving convolutional neural networks via concatenated rectiﬁed\\nlinear units,” in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\\ndetection,” in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll ´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[119] P. O. Pinheiro, R. Collobert, and P. Doll ´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n“The pascal visual object classes challenge 2012 (voc2012) results\\n(2012),” in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\\ntional networks,” in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll ´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, “Autocollage,”ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, “Autocollage,”ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, “Real-time salient object\\ndetection with a minimum spanning tree,” in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint crf and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576–588, 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Recognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353–367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989–1005, 2009.\\n[133] S. Xie and Z. Tu, “Holistically-nested edge detection,” in ICCV, 2015.\\n[134] M. K ¨ummerer, L. Theis, and M. Bethge, “Deep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,”\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='arXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency\\ndetection via local estimation and global search,” in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, “Weakly supervised top-down\\nsalient object detection,” arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by\\nmulti-context deep learning,” in CVPR, 2015.\\n[139] C ¸ . Bak, A. Erdem, and E. Erdem, “Two-stream convolutional networks\\nfor dynamic saliency prediction,” arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,”\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Int. J. of Comput. Vision , vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, “Deepsaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919–3930, 2016.\\n[142] Y . Tang and X. Wu, “Saliency detection via combining region-level\\nand pixel-level predictions with cnns,” in ECCV, 2016.\\n[143] G. Li and Y . Yu, “Deep contrast learning for salient object detection,”\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, “Edge preserving and\\nmulti-scale contextual neural network for salient object detection,”\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level\\nnetwork for saliency prediction,” in ICPR, 2016.\\n[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor,\\n“Shallow and deep convolutional networks for saliency prediction,” in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for\\nsaliency detection,” in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, “Deeply-supervised recurrent convolutional\\nneural network for saliency detection,” in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, “Contextual\\nhypergraph modeling for salient object detection,” in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\\ncontrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, “Salient object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='contrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, “Salient object\\ndetection: A discriminative regional feature integration approach,” in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, “Deep saliency with encoded low level\\ndistance map and high level features,” in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n“Non-local deep features for salient object detection,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n“Deeply supervised salient object detection with short connections,”\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,” in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of\\nsalient object segmentation,” in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, “Design and perceptual validation of\\nperformance measures for salient object segmentation,” in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:\\nA benchmark,” IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706–\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, “Face recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,” in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, “Face sketchcphoto synthesis\\nand retrieval using sparse representation,” IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213–1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, “A comprehensive survey\\nto face hallucination,” Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9–30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, “Multiple\\nrepresentations-based face sketch-photo synthesis.”IEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Netw. & Learning Syst. , vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial\\nexpression recognition system using deep network-based data fusion,”\\nIEEE Trans. Cybern. , vol. 48, pp. 103–114, 2018.\\n[166] P. Viola and M. Jones, “Robust real-time face detection,” Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, “Unitbox: An advanced\\nobject detection network,” in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, “Multi-view face detection\\nusing deep convolutional neural networks,” in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “From facial parts responses\\nto face detection: A deep learning approach,” in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, “Face detection through\\nscale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face\\ndetection,” in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, “Face r-cnn,” arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, “Face detection using deep learning: An\\nimproved faster rcnn approach,” arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, “Densebox: Unifying landmark\\nlocalization with end to end object detection,” arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, “face detection with end-to-end\\nintegration of a convnet and a 3d model,” in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, “Joint face detection and\\nalignment using multitask cascaded convolutional networks,” IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Signal Process. Lett. , vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, “Joint training of cascaded cnn for\\nface detection,” in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, “Fddb: A benchmark for face detection\\nin unconstrained settings,” Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A convolutional neural\\nnetwork cascade for face detection,” in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Aggregate channel features for\\nmulti-view face detection,” in IJCB, 2014.\\n[182] N. Marku ˇs, M. Frljak, I. S. Pand ˇzi´c, J. Ahlberg, and R. Forchheimer,\\n“Object detection with pixel intensity comparisons organized in deci-\\nsion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.\\n[184] J. Li and Y . Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, “A fast and accurate unconstrained\\nface detector,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211–223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,”\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, “Hyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,” arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.\\n[190] D. Gavrila and S. Munder, “Multi-cue pedestrian detection and tracking\\nfrom a moving vehicle,” Int. J. of Comput. Vision , vol. 73, pp. 41–59,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, “Jointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiﬁcation,” in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, “Stepwise metric promotion for unsuper-\\nvised video person re-identiﬁcation,” in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, “Cooperative robots to observe\\nmoving targets: Review,” IEEE Trans. Cybern. , vol. 48, pp. 187–198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res. , vol. 32, pp. 1231–1237,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res. , vol. 32, pp. 1231–1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, “Learning complexity-aware\\ncascades for deep pedestrian detection,” in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in CVPR, 2015.\\n[197] P. Doll ´ar, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids\\nfor object detection,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532–1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, “Filtered channel features for\\npedestrian detection,” in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, “Pedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained\\nand-or graph models for object shape detection,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959–972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, “Handling\\nocclusions with franken-classiﬁers,” in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, “Detection and tracking of\\noccluded people,” Int. J. of Comput. Vision, vol. 110, pp. 58–69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, “Is faster r-cnn doing well for\\npedestrian detection?” in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, “Multispectral deep\\nneural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by\\ndeep learning semantic tasks,” in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, “Fused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,” in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, “Pushing\\nthe limits of deep cnns for pedestrian detection,” IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom ´e, L. Bondi, L. Barofﬁo, S. Tubaro, E. Plebani, and D. Pau,\\n“Reduced memory region based deep convolutional neural network\\ndetection,” in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a deeper\\nlook at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='look at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, “Visual-textual\\njoint relevance learning for tag-based social image search,”IEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363–376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, “Ron: Reverse\\nconnection with objectness prior networks for object detection,” in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, “Generative adversarial nets,”\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, “Object\\ndetection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, “Learning detection with diverse\\nproposals,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end\\nmemory networks,” in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, “Real time image saliency for black box\\nclassiﬁers,” in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Craft objects from images,” in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, “Unsupervised learning\\nfrom video to detect foreground objects in single images,” in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, “Weakly supervised object\\nlocalization with latent category learning,” in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n“Training object class detectors with click supervision,” inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy\\ntrade-offs for modern convolutional object detectors,” in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, “Mimicking very efﬁcient network for object\\ndetection,” in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, “Fitnets: Hints for thin deep nets,” Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, “3d object proposals for accurate object class detection,”\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n“Object detection in videos with tubelet proposal networks,” in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='His research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor’s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Artiﬁcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='formation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2025. All\\nrights reserved. Draft of August 24, 2025.\\nCHAPTER\\n6\\nNeural Networks\\n“[M]achines of this character can behave in a very complicated manner when\\nthe number of units is large.”\\nAlan Turing (1948) “Intelligent Machines”, page 6\\nNeural networks are a fundamental computational tool for language process-\\ning, and a very old one. They are called neural because their origins lie in the\\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the\\nbiological neuron as a kind of computing element that could be described in terms\\nof propositional logic. But the modern use in language processing no longer draws\\non these early biological inspirations.\\nInstead, a modern neural network is a network of small computing units, each\\nof which takes a vector of input values and produces a single output value. In this\\nchapter we introduce the neural net applied to classiﬁcation. The architecture we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='of which takes a vector of input values and produces a single output value. In this\\nchapter we introduce the neural net applied to classiﬁcation. The architecture we\\nintroduce is called a feedforward network because the computation proceeds iter-feedforward\\natively from one layer of units to the next. The use of modern neural nets is often\\ncalled deep learning, because modern networks are often deep (have many layers).deep learning\\nNeural networks share much of the same mathematics as logistic regression. But\\nneural networks are a more powerful classiﬁer than logistic regression, and indeed a\\nminimal neural network (technically one with a single ‘hidden layer’) can be shown\\nto learn any function.\\nNeural net classiﬁers are different from logistic regression in another way. With\\nlogistic regression, we applied the regression classiﬁer to many different tasks by\\ndeveloping many rich kinds of feature templates based on domain knowledge. When'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='logistic regression, we applied the regression classiﬁer to many different tasks by\\ndeveloping many rich kinds of feature templates based on domain knowledge. When\\nworking with neural networks, it is more common to avoid most uses of rich hand-\\nderived features, instead building neural networks that take raw tokens as inputs\\nand learn to induce features as part of the process of learning to classify. We saw\\nexamples of this kind of representation learning for embeddings in Chapter 5, and\\nwe’ll see lots of examples once we start studying deep transformers networks. Nets\\nthat are very deep are particularly good at representation learning. For that reason\\ndeep neural nets are the right tool for tasks that offer sufﬁcient data to learn features\\nautomatically.\\nIn this chapter we’ll introduce feedforward networks as classiﬁers, ﬁrst with\\nhand-built features, and then using the embeddings that we studied in Chapter 5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='automatically.\\nIn this chapter we’ll introduce feedforward networks as classiﬁers, ﬁrst with\\nhand-built features, and then using the embeddings that we studied in Chapter 5.\\nIn subsequent chapters we’ll introduce many other kinds of neural models, most\\nimportantly the transformer and attention, (Chapter 8), but also recurrent neural\\nnetworks (Chapter 13) and convolutional neural networks (Chapter 15). And in\\nthe next chapter we’ll introduce the paradigm of neural large language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2 CHAPTER 6 • N EURAL NETWORKS\\n6.1 Units\\nThe building block of a neural network is a single computational unit. A unit takes\\na set of real valued numbers as input, performs some computation on them, and\\nproduces an output.\\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\\ntional term in the sum called a bias term. Given a set of inputs x1...xn, a unit hasbias term\\na set of corresponding weights w1...wn and a bias b, so the weighted sum z can be\\nrepresented as:\\nz = b +\\n∑\\ni\\nwixi (6.1)\\nOften it’s more convenient to express this weighted sum using vector notation; recall\\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thusvector\\nwe’ll talk aboutz in terms of a weight vector w, a scalar bias b, and an input vector\\nx, and we’ll replace the sum with the convenientdot product:\\nz = w ·x+b (6.2)\\nAs deﬁned in Eq. 6.2, z is just a real valued number.\\nFinally, instead of using z, a linear function of x, as the output, neural units'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='z = w ·x+b (6.2)\\nAs deﬁned in Eq. 6.2, z is just a real valued number.\\nFinally, instead of using z, a linear function of x, as the output, neural units\\napply a non-linear function f to z. We will refer to the output of this function as\\nthe activation value for the unit, a. Since we are just modeling a single unit, theactivation\\nactivation for the node is in fact the ﬁnal output of the network, which we’ll generally\\ncall y. So the value y is deﬁned as:\\ny = a = f (z)\\nWe’ll discuss three popular non-linear functionsf below (the sigmoid, the tanh, and\\nthe rectiﬁed linear unit or ReLU) but it’s pedagogically convenient to start with the\\nsigmoid function since we saw it in Chapter 4:sigmoid\\ny = σ(z) = 1\\n1 +e−z (6.3)\\nThe sigmoid (shown in Fig. 6.1) has a number of advantages; it maps the output\\ninto the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\\ndifferentiable, which as we saw in Section ?? will be handy for learning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='into the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\\ndifferentiable, which as we saw in Section ?? will be handy for learning.\\nFigure 6.1 The sigmoid function takes a real value and maps it to the range (0,1). It is\\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\\nSubstituting Eq. 6.2 into Eq. 6.3 gives us the output of a neural unit:\\ny = σ(w ·x+b) = 1\\n1 +exp(−(w ·x+b)) (6.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.1 • U NITS 3\\nFig. 6.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit\\ntakes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each\\nvalue by a weight (w1, w2, and w3, respectively), adds them to a bias termb, and then\\npasses the resulting sum through a sigmoid function to result in a number between 0\\nand 1.\\nx1\\nx2\\nx3\\ny\\nw1\\nw2\\nw3\\n∑\\nb\\nσ\\n+1\\nz a\\nFigure 6.2 A neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a\\nweight for an input clamped at +1) and producing an output y. We include some convenient\\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\\nthis case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to\\nmean the ﬁnal output of the entire network, leaving a as the activation of an individual node.\\nLet’s walk through an example just to get an intuition. Let’s suppose we have a\\nunit with the following weight vector and bias:\\nw = [0.2,0.3,0.9]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Let’s walk through an example just to get an intuition. Let’s suppose we have a\\nunit with the following weight vector and bias:\\nw = [0.2,0.3,0.9]\\nb = 0.5\\nWhat would this unit do with the following input vector:\\nx = [0.5,0.6,0.1]\\nThe resulting output y would be:\\ny = σ(w ·x+b) = 1\\n1 +e−(w·x+b) = 1\\n1 +e−(.5∗.2+.6∗.3+.1∗.9+.5) = 1\\n1 +e−0.87 = .70\\nIn practice, the sigmoid is not commonly used as an activation function. A function\\nthat is very similar but almost always better is the tanh function shown in Fig. 6.3a;tanh\\ntanh is a variant of the sigmoid that ranges from -1 to +1:\\ny = tanh(z) =ez −e−z\\nez +e−z (6.5)\\nThe simplest activation function, and perhaps the most commonly used, is the rec-\\ntiﬁed linear unit, also called the ReLU, shown in Fig. 6.3b. It’s just the same as zReLU\\nwhen z is positive, and 0 otherwise:\\ny = ReLU(z) = max(z,0) (6.6)\\nThese activation functions have different properties that make them useful for differ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='when z is positive, and 0 otherwise:\\ny = ReLU(z) = max(z,0) (6.6)\\nThese activation functions have different properties that make them useful for differ-\\nent language applications or network architectures. For example, the tanh function\\nhas the nice properties of being smoothly differentiable and mapping outlier values\\ntoward the mean. The rectiﬁer function, on the other hand, has nice properties that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4 CHAPTER 6 • N EURAL NETWORKS\\n(a) (b)\\nFigure 6.3 The tanh and ReLU activation functions.\\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\\nvalues of z result in values ofy that are saturated, i.e., extremely close to 1, and havesaturated\\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\\nwe’ll see in Section 6.6, we’ll train networks by propagating an error signal back-\\nwards, multiplying gradients (partial derivatives) from each layer of the network;\\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\\ntoo small to be used for training, a problem called the vanishing gradient problem.vanishing\\ngradient\\nRectiﬁers don’t have this problem, since the derivative of ReLU for high values ofz\\nis 1 rather than very close to 0.\\n6.2 The XOR problem\\nEarly in the history of neural networks it was realized that the power of neural net-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='is 1 rather than very close to 0.\\n6.2 The XOR problem\\nEarly in the history of neural networks it was realized that the power of neural net-\\nworks, as with the real neurons that inspired them, comes from combining these\\nunits into larger networks.\\nOne of the most clever demonstrations of the need for multi-layer networks was\\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\\nsome very simple functions of its input. Consider the task of computing elementary\\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\\nthe truth tables for those functions:\\nAND OR XOR\\nx1 x2 y x1 x2 y x1 x2 y\\n0 0 0 0 0 0 0 0 0\\n0 1 0 0 1 1 0 1 1\\n1 0 0 1 0 1 1 0 1\\n1 1 1 1 1 1 1 1 0\\nThis example was ﬁrst shown for the perceptron, which is a very simple neuralperceptron\\nunit that has a binary output and has a very simple step function as its non-linear\\nactivation function. The output y of a perceptron is 0 or 1, and is computed as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='unit that has a binary output and has a very simple step function as its non-linear\\nactivation function. The output y of a perceptron is 0 or 1, and is computed as\\nfollows (using the same weight w, input x, and bias b as in Eq. 6.2):\\ny =\\n{0, if w ·x+b ≤0\\n1, if w ·x+b > 0 (6.7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.2 • T HE XOR PROBLEM 5\\nIt’s very easy to build a perceptron that can compute the logical AND and OR\\nfunctions of its binary inputs; Fig. 6.4 shows the necessary weights.\\nx1\\nx2\\n+1\\n-1\\n1\\n1\\nx1\\nx2\\n+1\\n0\\n1\\n1\\n(a) (b)\\nFigure 6.4 The weights w and bias b for perceptrons for computing logical functions. The\\ninputs are shown asx1 and x2 and the bias as a special node with value+1 which is multiplied\\nwith the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight\\nb = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\\nweights/biases are just one from an inﬁnite number of possible sets of weights and biases that\\nwould implement the functions.\\nIt turns out, however, that it’s not possible to build a perceptron to compute\\nlogical XOR! (It’s worth spending a moment to give it a try!)\\nThe intuition behind this important result relies on understanding that a percep-\\ntron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='The intuition behind this important result relies on understanding that a percep-\\ntron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron\\nequation, w1x1 +w2x2 +b = 0 is the equation of a line. (We can see this by putting\\nit in the standard linear format: x2 = (−w1/w2)x1 + (−b/w2).) This line acts as a\\ndecision boundary in two-dimensional space in which the output 0 is assigned to alldecision\\nboundary\\ninputs lying on one side of the line, and the output 1 to all input points lying on the\\nother side of the line. If we had more than 2 inputs, the decision boundary becomes\\na hyperplane instead of a line, but the idea is the same, separating the space into two\\ncategories.\\nFig. 6.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\\nby one possible set of parameters for an AND and an OR classiﬁer. Notice that there\\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='by one possible set of parameters for an AND and an OR classiﬁer. Notice that there\\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\\nseparable\\nfunction. Of course we could draw a boundary with a curve, or some other function,\\nbut not a single line.\\n6.2.1 The solution: neural networks\\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\\nculated by a layered network of perceptron units. Rather than see this with networks\\nof simple perceptrons, however, let’s see how to compute XOR using two layers of\\nReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a ﬁgure with\\nthe input being processed by two layers of neural units. The middle layer (called\\nh) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='h) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.\\nLet’s walk through what happens with the input x = [0, 0]. If we multiply each\\ninput value by the appropriate weight, sum, and then add the biasb, we get the vector\\n[0, -1], and we then apply the rectiﬁed linear transformation to give the output of the\\nh layer as [0, 0]. Now we once again multiply by the weights, sum, and add the\\nbias (0 in this case) resulting in the value 0. The reader should work through the\\ncomputation of the remaining 3 possible input pairs to see that the resultingy values\\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6 CHAPTER 6 • N EURAL NETWORKS\\n0\\n0 1\\n1\\nx1\\nx2\\n0\\n0 1\\n1\\nx1\\nx2\\n0\\n0 1\\n1\\nx1\\nx2\\na)  x1 AND x2 b)  x1 OR x2 c)  x1 XOR x2\\n?\\nFigure 6.5 The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the\\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\\n(2002).\\nx1\\nx2\\nh1\\nh2\\ny1\\n+1\\n1\\n-1\\n1\\n1\\n1\\n-2\\n0\\n1\\n+1\\n0\\nFigure 6.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\\ntwo layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers\\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\\non a unit clamped to +1, with the bias weights/units in gray.\\nIt’s also instructive to look at the intermediate results, the outputs of the two\\nhidden nodes h1 and h2. We showed in the previous paragraph that the h vector for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='It’s also instructive to look at the intermediate results, the outputs of the two\\nhidden nodes h1 and h2. We showed in the previous paragraph that the h vector for\\nthe inputs x = [0, 0] was [0, 0]. Fig. 6.7b shows the values of the h layer for all\\n4 inputs. Notice that hidden representations of the two input points x = [0, 1] and\\nx = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =\\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\\nof XOR. In other words, we can view the hidden layer of the network as forming a\\nrepresentation of the input.\\nIn this example we just stipulated the weights in Fig. 6.6. But for real examples\\nthe weights for neural networks are learned automatically using the error backprop-\\nagation algorithm to be introduced in Section 6.6. That means the hidden layers will\\nlearn to form useful representations. This intuition, that neural networks can auto-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='agation algorithm to be introduced in Section 6.6. That means the hidden layers will\\nlearn to form useful representations. This intuition, that neural networks can auto-\\nmatically learn useful representations of the input, is one of their key advantages,\\nand one that we will return to again and again in later chapters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.3 • F EEDFORWARD NEURAL NETWORKS 7\\n0\\n0 1\\n1\\nx1\\nx2\\na) The original x space\\n0\\n0 1\\n1\\nh1\\nh2\\n2\\nb) The new (linearly separable) h space\\nFigure 6.7 The hidden layer forming a new representation of the input. (b) shows the\\nrepresentation of the hidden layer, h, compared to the original input representation x in (a).\\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\\n(2016).\\n6.3 Feedforward Neural Networks\\nLet’s now walk through a slightly more formal presentation of the simplest kind of\\nneural network, the feedforward network. A feedforward network is a multilayerfeedforward\\nnetwork\\nnetwork in which the units are connected with no cycles; the outputs from units in\\neach layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='each layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)\\nFor historical reasons multilayer networks, especially feedforward networks, are\\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,multi-layer\\nperceptrons\\nMLP since the units in modern multilayer networks aren’t perceptrons (perceptrons have a\\nsimple step-function as their activation function, but modern networks are made up\\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\\npoint the name stuck.\\nSimple feedforward networks have three kinds of nodes: input units, hidden\\nunits, and output units.\\nFig. 6.8 shows a picture. The input layerx is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is thehidden layer h formed of hidden units hi,hidden layer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Fig. 6.8 shows a picture. The input layerx is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is thehidden layer h formed of hidden units hi,hidden layer\\neach of which is a neural unit as described in Section 6.1, taking a weighted sum of\\nits inputs and then applying a non-linearity. In the standard architecture, each layer\\nis fully-connected, meaning that each unit in each layer takes as input the outputsfully-connected\\nfrom all the units in the previous layer, and there is a link between every pair of units\\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\\nrepresent the parameters for the entire hidden layer by combining the weight vector\\nand bias for each unit i into a single weight matrix W and a single bias vector b for\\nthe whole layer (see Fig. 6.8). Each element Wji of the weight matrix W represents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='and bias for each unit i into a single weight matrix W and a single bias vector b for\\nthe whole layer (see Fig. 6.8). Each element Wji of the weight matrix W represents\\nthe weight of the connection from the ith input unit xi to the jth hidden unit hj.\\nThe advantage of using a single matrix W for the weights of the entire layer is\\nthat now the hidden layer computation for a feedforward network can be done very'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='8 CHAPTER 6 • N EURAL NETWORKS\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW\\ninput layer hidden layer output layer\\nh1\\ny1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nFigure 6.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\\nand one input layer (the input layer is usually not counted when enumerating layers).\\nefﬁciently with simple matrix operations. In fact, the computation only has three\\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\\nand applying the activation functiong (such as the sigmoid, tanh, or ReLU activation\\nfunction deﬁned above).\\nThe output of the hidden layer, the vectorh, is thus the following (for this exam-\\nple we’ll use the sigmoid functionσ as our activation function):\\nh = σ(Wx+b) (6.8)\\nNotice that we’re applying the σ function here to a vector, while in Eq. 6.3 it was\\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function\\ng(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='applied to a scalar. We’re thus allowing σ(·), and indeed any activation function\\ng(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].\\nLet’s introduce some constants to represent the dimensionalities of these vectors\\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have\\nn0 represent the number of inputs, so x is a vector of real numbers of dimension\\nn0, or more formally x ∈Rn0 , a column vector of dimensionality [n0 ×1]. Let’s\\ncall the hidden layer layer 1 and the output layer layer 2. The hidden layer has\\ndimensionality n1, so h ∈Rn1 and also b ∈Rn1 (since each hidden unit can take a\\ndifferent bias value). And the weight matrix W has dimensionality W ∈Rn1×n0 , i.e.\\n[n1 ×n0].\\nTake a moment to convince yourself that the matrix multiplication in Eq. 6.8 will\\ncompute the value of each hj as σ\\n(∑n0\\ni=1 Wjixi +bj\\n)\\n.\\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypoth-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='compute the value of each hj as σ\\n(∑n0\\ni=1 Wjixi +bj\\n)\\n.\\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypoth-\\nesis) forms a representation of the input. The role of the output layer is to take\\nthis new representation h and compute a ﬁnal output. This output could be a real-\\nvalued number, but in many cases the goal of the network is to make some sort of\\nclassiﬁcation decision, and so we will focus on the case of classiﬁcation.\\nIf we are doing a binary task like sentiment classiﬁcation, we might have a sin-\\ngle output node, and its scalar value y is the probability of positive versus negative\\nsentiment. If we are doing multinomial classiﬁcation, such as assigning a part-of-\\nspeech tag, we might have one output node for each potential part-of-speech, whose\\noutput value is the probability of that part-of-speech, and the values of all the output\\nnodes must sum to one. The output layer is thus a vector y that gives a probability'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='output value is the probability of that part-of-speech, and the values of all the output\\nnodes must sum to one. The output layer is thus a vector y that gives a probability\\ndistribution across the output nodes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.3 • F EEDFORWARD NEURAL NETWORKS 9\\nLet’s see how this happens. Like the hidden layer, the output layer has a weight\\nmatrix (let’s call it U), but some models don’t include a bias vector b in the output\\nlayer, so we’ll simplify by eliminating the bias vector in this example. The weight\\nmatrix is multiplied by its input vector (h) to produce the intermediate output z:\\nz = Uh\\nThere are n2 output nodes, so z ∈Rn2 , weight matrix U has dimensionality U ∈\\nRn2×n1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the\\noutput layer.\\nHowever, z can’t be the output of the classiﬁer, since it’s a vector of real-valued\\nnumbers, while what we need for classiﬁcation is a vector of probabilities. There is\\na convenient function for normalizing a vector of real values, by which we meannormalizing\\nconverting it to a vector that encodes a probability distribution (all the numbers lie\\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? ofsoftmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='converting it to a vector that encodes a probability distribution (all the numbers lie\\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? ofsoftmax\\nChapter 4. More generally for any vector z of dimensionality d, the softmax is\\ndeﬁned as:\\nsoftmax(zi) = exp(zi)∑d\\nj=1 exp(zj)\\n1 ≤i ≤d (6.9)\\nThus for example given a vector\\nz = [0.6,1.1,−1.5,1.2,3.2,−1.1], (6.10)\\nthe softmax function will normalize it to a probability distribution (shown rounded):\\nsoftmax(z) = [0.055,0.090,0.0067,0.10,0.74,0.010] (6.11)\\nYou may recall that we used softmax to create a probability distribution from a\\nvector of real-valued numbers (computed from summing weights times features) in\\nthe multinomial version of logistic regression in Chapter 4.\\nThat means we can think of a neural network classiﬁer with one hidden layer\\nas building a vector h which is a hidden layer representation of the input, and then\\nrunning standard multinomial logistic regression on the features that the network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='as building a vector h which is a hidden layer representation of the input, and then\\nrunning standard multinomial logistic regression on the features that the network\\ndevelops in h. By contrast, in Chapter 4 the features were mainly designed by hand\\nvia feature templates. So a neural network is like multinomial logistic regression,\\nbut (a) with many layers, since a deep neural network is like layer after layer of lo-\\ngistic regression classiﬁers; (b) with those intermediate layers having many possible\\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll\\ncontinue to use σ for convenience to mean any activation function); (c) rather than\\nforming the features by feature templates, the prior layers of the network induce the\\nfeature representations themselves.\\nHere are the ﬁnal equations for a feedforward network with a single hidden layer,\\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Here are the ﬁnal equations for a feedforward network with a single hidden layer,\\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\\nized by weight matrices W and U and a bias vector b:\\nh = σ(Wx+b)\\nz = Uh\\ny = softmax(z) (6.12)\\nAnd just to remember the shapes of all our variables, x ∈Rn0 , h ∈Rn1 , b ∈Rn1 ,\\nW ∈Rn1×n0 , U ∈Rn2×n1 , and the output vectory ∈Rn2 . We’ll call this network a 2-\\nlayer network (we traditionally don’t count the input layer when numbering layers,\\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\\nnetwork.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='10 CHAPTER 6 • N EURAL NETWORKS\\n6.3.1 More details on feedforward networks\\nLet’s now set up some notation to make it easier to talk about deeper networks of\\ndepth more than 2. We’ll use superscripts in square brackets to mean layer num-\\nbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\\n(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. nj\\nwill mean the number of units at layer j. We’ll use g(·) to stand for the activation\\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\\nfor output layers. We’ll usea[i] to mean the output from layer i, and z[i] to mean the\\ncombination of previous layer output, weights and biases W[i]a[i−1] + b[i]. The 0th\\nlayer is for inputs, so we’ll refer to the inputsx more generally as a[0].\\nThus we can re-represent our 2-layer net from Eq. 6.12 as follows:\\nz[1] = W[1]a[0] +b[1]\\na[1] = g[1](z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = g[2](z[2])\\nˆy = a[2] (6.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Thus we can re-represent our 2-layer net from Eq. 6.12 as follows:\\nz[1] = W[1]a[0] +b[1]\\na[1] = g[1](z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = g[2](z[2])\\nˆy = a[2] (6.13)\\nNote that with this notation, the equations for the computation done at each layer are\\nthe same. The algorithm for computing the forward step in an n-layer feedforward\\nnetwork, given the input vector a[0] is thus simply:\\nfor i in 1,...,n\\nz[i] = W[i] a[i−1] + b[i]\\na[i] = g[i](z[i])\\nˆy = a[n]\\nIt’s often useful to have a name for the ﬁnal set of activations right before the ﬁnal\\nsoftmax. So however many layers we have, we’ll generally call the unnormalized\\nvalues in the ﬁnal vector z[n], the vector of scores right before the ﬁnal softmax, the\\nlogits (see Eq. ??).logits\\nThe need for non-linear activation functions One of the reasons we use non-\\nlinear activation functions for each layer in a neural network is that if we did not, the\\nresulting network is exactly equivalent to a single-layer network. Let’s see why this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='linear activation functions for each layer in a neural network is that if we did not, the\\nresulting network is exactly equivalent to a single-layer network. Let’s see why this\\nis true. Imagine the ﬁrst two layers of such a network of purely linear layers:\\nz[1] = W[1]x+b[1]\\nz[2] = W[2]z[1] +b[2]\\nWe can rewrite the function that the network is computing as:\\nz[2] = W[2]z[1] +b[2]\\n= W[2](W[1]x+b[1])+ b[2]\\n= W[2]W[1]x+W[2]b[1] +b[2]\\n= W′x+b′ (6.14)\\nThis generalizes to any number of layers. So without non-linear activation functions,\\na multilayer network is just a notational variant of a single layer network with a\\ndifferent set of weights, and we lose all the representational power of multilayer\\nnetworks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.4 • F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 11\\nReplacing the bias unit In describing networks, we will sometimes use a slightly\\nsimpliﬁed notation that represents exactly the same function without referring to an\\nexplicit bias node b. Instead, we add a dummy node a0 to each layer whose value\\nwill always be 1. Thus layer 0, the input layer, will have a dummy node a[0]\\n0 = 1,\\nlayer 1 will havea[1]\\n0 = 1, and so on. This dummy node still has an associated weight,\\nand that weight represents the bias value b. For example instead of an equation like\\nh = σ(Wx+b) (6.15)\\nwe’ll use:\\nh = σ(Wx) (6.16)\\nBut now instead of our vector x having n0 values: x = x1,..., xn0 , it will have n0 +\\n1 values, with a new 0th dummy value x0 = 1: x = x0,..., xn0 . And instead of\\ncomputing each hj as follows:\\nhj = σ\\n(n0∑\\ni=1\\nWji xi +bj\\n)\\n, (6.17)\\nwe’ll instead use:\\nhj = σ\\n(n0∑\\ni=0\\nWji xi\\n)\\n, (6.18)\\nwhere the value Wj0 replaces what had been bj. Fig. 6.9 shows a visualization.\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW h1 y1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='hj = σ\\n(n0∑\\ni=1\\nWji xi +bj\\n)\\n, (6.17)\\nwe’ll instead use:\\nhj = σ\\n(n0∑\\ni=0\\nWji xi\\n)\\n, (6.18)\\nwhere the value Wj0 replaces what had been bj. Fig. 6.9 shows a visualization.\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW h1 y1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nx1\\nx2\\nxn0\\n…\\n…\\nx0=1\\n…\\nUW\\nh1 y1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\n(a) (b)\\nFigure 6.9 Replacing the bias node (shown in a) with x0 (b).\\nWe’ll continue showing the bias as b when we go over the learning algorithm\\nin Section 6.6, but going forward in the book, for most ﬁgures and some equations\\nwe’ll use this simpliﬁed notation without explicit bias terms.\\n6.4 Feedforward networks for NLP: Classiﬁcation\\nLet’s see how to apply feedforward networks to NLP classiﬁcation tasks. In practice,\\nsimple feedforward networks aren’t the way we do text classiﬁcation; for real appli-\\ncations we would use more sophisticated architectures like the BERT transformers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='12 CHAPTER 6 • N EURAL NETWORKS\\nof Chapter 10. Nonetheless seeing a feedforward network text classiﬁer will let us\\nintroduce key ideas that will play a role throughout the rest of the book, includ-\\ning the ideas of theembedding matrix, representation pooling, and representation\\nlearning.\\nBut before introducing any of these ideas, let’s start with a classiﬁer by making\\nonly minimal change from the sentiment classiﬁers we saw in Chapter 4. Like them,\\nwe’ll take hand-built features, pass them through a classiﬁer, and produce a class\\nprobability. The only difference is that we’ll use a neural network instead of logistic\\nregression as the classiﬁer.\\n6.4.1 Neural net classiﬁers with hand-built features\\nLet’s begin with a simple 2-layer sentiment classiﬁer by taking our logistic regres-\\nsion classiﬁer from Chapter 4, which corresponds to a 1-layer network, and just\\nadding a hidden layer. The input element xi can be scalar features like those in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sion classiﬁer from Chapter 4, which corresponds to a 1-layer network, and just\\nadding a hidden layer. The input element xi can be scalar features like those in\\nFig. ??, e.g., x1 = count(words ∈doc), x2 = count(positive lexicon words ∈doc),\\nx3 = 1 if “no” ∈doc, and so on, for a total of d features. And the output layer\\nˆy could have two nodes (one each for positive and negative), or 3 nodes (positive,\\nnegative, neutral), in which case ˆy1 would be the estimated probability of positive\\nsentiment, ˆy2 the probability of negative and ˆy3 the probability of neutral. The re-\\nsulting equations would be just what we saw above for a 2-layer network (as always,\\nwe’ll continue to use the σ to stand for any non-linearity, whether sigmoid, ReLU\\nor other).\\nx = [x1,x2,...xd] (each xi is a hand-designed feature)\\nh = σ(Wx+b)\\nz = Uh\\nˆy = softmax(z) (6.19)\\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='or other).\\nx = [x1,x2,...xd] (each xi is a hand-designed feature)\\nh = σ(Wx+b)\\nz = Uh\\nˆy = softmax(z) (6.19)\\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this\\nhidden layer to our logistic regression classiﬁer allows the network to represent the\\nnon-linear interactions between features. This alone might give us a better sentiment\\nclassiﬁer.\\nUW\\n[d⨉1]\\nHidden layer Output layer\\nsoftmax\\n[dh⨉d] [dh⨉1] [3⨉dh]\\nInput words\\np(+)\\nh1\\nh2\\nh3\\nhdh …\\ny1^\\ny2^\\ny3^\\nx h y\\nInput layer \\nd=3 features\\n[3⨉1]\\nx1\\nx2\\nx3\\ndessert\\nwas\\ngreat\\npositive lexicon\\nwords = 1\\ncount of “no” \\n= 0\\nwordcount\\n=3\\np(-)\\np(neut)\\nFigure 6.10 Feedforward network sentiment analysis using traditional hand-built features\\nof the input text.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 13\\n6.4.2 Vectorizing for parallelizing inference\\nWhile Eq. 6.19 shows how to classify a single example x, in practice we want to\\nefﬁciently classify an entire test set of m examples. We do this by vectorizing the\\nprocess, just as we saw with logistic regression; instead of using for-loops to go\\nthrough each example, we’ll use matrix multiplication to do the entire computation\\nof an entire test set at once. First, we pack all the input feature vectors for each input\\nx into a single input matrixX, with each rowi a row vector consisting of the features\\nfor input example x(i) (i.e., the vector x(i)). If the dimensionality of our input feature\\nvector is d, X will be a matrix of shape [m ×d].\\nBecause we are now modeling each input as a row vector rather than a column\\nvector, we also need to slightly modify Eq. 6.19. X is of shape [m ×d] and W is of\\nshape [dh ×d], so we’ll reorder how we multiplyX and W and transpose W so they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='vector, we also need to slightly modify Eq. 6.19. X is of shape [m ×d] and W is of\\nshape [dh ×d], so we’ll reorder how we multiplyX and W and transpose W so they\\ncorrectly multiply to yield a matrix H of shape [m ×dh]. 1\\nThe bias vector b from Eq. 6.19 of shape [1 ×dh] will now have to be replicated\\ninto a matrix of shape [m ×dh]. We’ll need to similarly reorder the next step and\\ntranspose U. Finally, our output matrix ˆY will be of shape [m ×3] (or more gen-\\nerally [m ×do], where do is the number of output classes), with each row i of our\\noutput matrix ˆY consisting of the output vector ˆy(i). Here are the ﬁnal equations for\\ncomputing the output class distribution for an entire test set:\\nH = σ(XW⊺ +b)\\nZ = HU⊺\\nˆY = softmax(Z) (6.20)\\nIn this book, we’ll sometimes see orderings like WX + b and sometimes XW + b.\\nThat’s why it’s always important to be very aware of the shapes of your weight\\nmatrices participating in any given equation.\\n6.5 Embeddings as the input to neural net classiﬁers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='That’s why it’s always important to be very aware of the shapes of your weight\\nmatrices participating in any given equation.\\n6.5 Embeddings as the input to neural net classiﬁers\\nWhile hand-built features are a traditional way to design classiﬁers, most applica-\\ntions of neural networks for NLP don’t use hand-built human-engineered features as\\ninputs. Instead, we draw on deep learning’s ability to learn features from the data by\\nrepresenting tokens as embeddings. For this section we’ll represent each token by\\nits static word2vec or GloVe embeddings that we saw how to compute in Chapter 5.\\nBy static embedding, we mean that each token is represented by a ﬁxed vector that\\nwe train once, and then just put into a big dictionary. When we want to refer to that\\ntoken, we grab its embedding out of the dictionary.\\nHowever when we apply neural models to the task of language modeling (as\\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more power-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='However when we apply neural models to the task of language modeling (as\\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more power-\\nful kind of embedding called a contextual embedding. Contextual embeddings are\\ndifferent for each time a word occurs in a different context. Furthermore, we’ll have\\nthe network learn these embeddings as part of the task of word prediction.\\nSo let’s explore the text classiﬁcation domain above, but using static embeddings\\nas features instead of the hand-designed features. Let’s focus on the inference stage,\\n1 Note that we could have kept the original order of our products if we had instead made our input\\nmatrix X represent each input as a column vector instead of a row vector, making it of shape[d ×m]. But\\nrepresenting inputs as row vectors is convenient and common in neural network models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='14 CHAPTER 6 • N EURAL NETWORKS\\nin which we have already learned embeddings for all the input tokens. An embed-\\nding is a vector of dimension d that represents the input token. The dictionary of\\nstatic embeddings in which we store these embeddings is the embedding matrixembedding\\nmatrix\\nE. Each row of the embedding matrix represents each token of the vocabulary V\\nas a (row) vector of dimensionality d. Since E has a row for each of the |V |to-\\nkens in the vocabulary, E has shape [|V |×d]. This embedding matrix E plays a role\\nwhenever we are using embeddings as input to neural NLP systems, including in the\\ntransformer-based large language models we will introduce over the next chapters.\\nGiven an input token string likedessert was great we ﬁrst convert the tokens\\ninto vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of dessert was great might be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='into vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of dessert was great might be\\nw = [3,9824,226]. Next we use indexing to select the corresponding rows from E\\n(row 3, row 4000, row 10532).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent input tokens as one-hot vectors of shape [1 ×|V |], i.e., with\\none dimension for each word in the vocabulary. Recall that in a one-hot vector allone-hot vector\\nthe elements are 0 except one, the element whose dimension is the word’s index\\nin the vocabulary, which has value 1. So if the word “dessert” has index 3 in the\\nvocabulary, x3 = 1, and xi = 0 ∀i ̸= 3, as shown here:\\n[0 0 1 0 0 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ... ... |V|\\nMultiplying by a one-hot vector that has only one non-zero elementxi = 1 simply\\nselects out the relevant row vector for wordi, resulting in the embedding for word i,\\nas depicted in Fig. 6.11.\\nE\\n|V|\\nd\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='selects out the relevant row vector for wordi, resulting in the embedding for word i,\\nas depicted in Fig. 6.11.\\nE\\n|V|\\nd\\n1\\n|V| d\\n=✕\\n33\\n0 0 1 0 0 0 0 … 0 0 0 0 1\\nFigure 6.11 Selecting the embedding vector for word V3 by multiplying the embedding\\nmatrix E with a one-hot vector with a 1 in index 3.\\nWe can extend this idea to represent the entire input token sequence as a matrix\\nof one-hot vectors, one for each of the N input positions as shown in Fig. 6.12.\\nE\\n|V|\\nd\\nd\\nN\\n=✕\\n|V|\\nN\\n0 0 0 0 0 0 0 … 0 0 1 0 \\n0 0 1 0 0 0 0 … 0 0 0 0 \\n1 0 0 0 0 0 0 … 0 0 0 0 \\n0 0 0 0 1 0 0 … 0 0 0 0 \\n…\\nFigure 6.12 Selecting the embedding matrix for the input sequence of token idsW by mul-\\ntiplying a one-hot matrix corresponding to W by the embedding matrix E.\\nWe now need to classify this input ofN [1 ×d] embeddings, representing a win-\\ndow of N tokens, into a single class (like positive or negative).\\nThere are two common ways to to pass embeddings to a classiﬁer: concate-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dow of N tokens, into a single class (like positive or negative).\\nThere are two common ways to to pass embeddings to a classiﬁer: concate-\\nnation and pooling. First, we can take this input of shape [N ×d] and reshape it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 15\\nby concatenating all the input vectors into one very long vector of shape [1 ×dN].\\nThen we pass this input to our classiﬁer and let it make its decision. This gives\\nus lots of information, at the cost of using a pretty large network. Second, we can\\npool the N embeddings into a single embedding and then pass that single pooledpool\\nembedding to the classiﬁer. Pooling gives us less information than would have been\\npresent in all the original embeddings, but has the advantage of being small and ef-\\nﬁcient and is especially useful in tasks for which we don’t care as much about the\\noriginal word order. Let’s give an example of each: pooling for the sentiment task,\\nand concatenation for the language modeling task.\\nPooling input embeddings for sentiment So let’s begin with seeing how pooling\\ncan work for the sentiment classiﬁcation task. The intuition of pooling is that for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Pooling input embeddings for sentiment So let’s begin with seeing how pooling\\ncan work for the sentiment classiﬁcation task. The intuition of pooling is that for\\nsentiment, the exact position of the input (is some word like great the ﬁrst word?\\nthe second word?) is less important than the identity of the word itself.\\nA pooling function is a way to turn a set of embeddings into a single embedding.\\nFor example, for a text with N input words/tokens w1,..., wN , we want to turn\\nthe N row embeddings e(w1),..., e(wN ) (each of dimensionality d) into a single\\nembedding also of dimensionality d.\\nThere are various ways to pool. The simplest is mean-pooling: taking the meanmean-pooling\\nby summing the embeddings and then dividing by N:\\nxmean = 1\\nN\\nN∑\\ni=1\\ne(wi) (6.21)\\nHere are the equations for this classiﬁer assuming mean pooling:\\nx = mean(e(w1),e(w2),..., e(wn))\\nh = σ(xW +b)\\nz = hU\\nˆy = softmax(z) (6.22)\\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='x = mean(e(w1),e(w2),..., e(wn))\\nh = σ(xW +b)\\nz = hU\\nˆy = softmax(z) (6.22)\\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the\\nrelevant matrices.\\nThere are many other options for pooling, like max-pooling, in which case formax-pooling\\neach dimension we take the element-wise max over all the inputs. The element-wise\\nmax of a set of N vectors is a new vector whose kth element is the max of the kth\\nelements of all the N vectors.\\nConcatenating input embeddings for language modeling For sentiment analy-\\nsis we saw how to generate an output vector with probabilities over three classes:\\npositive, negative, or neutral, given as input a window of N input tokens, by ﬁrst\\npooling those token embeddings into a single embedding vector.\\nNow let’s considerlanguage modeling: predicting upcoming words from prior\\nwords. In this task we are given the same window of N input tokens, but our task'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Now let’s considerlanguage modeling: predicting upcoming words from prior\\nwords. In this task we are given the same window of N input tokens, but our task\\nnow is to predict the next token that should follow the window. We’ll sketch a\\nsimple feedforward neural language model, drawing on an algorithm ﬁrst introduced\\nby Bengio et al. (2003). The feedforward language model introduces many of the\\nimportant concepts of large language modeling that we will return to in Chapter 7\\nand Chapter 8.\\nNeural language models have many advantages over the n-gram language mod-\\nels of Chapter 3. Neural language models can handle much longer histories, can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='16 CHAPTER 6 • N EURAL NETWORKS\\n“dessert” = V3 “was” = V524 “great” = V902\\nembedding for “dessert”\\nembedding for “was”\\nembedding for “great”\\nU\\nW\\n[1⨉d]\\nHidden layer\\nOutput layer\\n[d⨉dh]\\n[1⨉dh]\\n[dh⨉3]\\nInput words\\np(+)\\nh1 h2 h3 hdh\\n…\\ny1\\n^ y2^ y3^\\nx\\nh\\ny\\nInput layer \\n[1⨉3]\\npooling+\\np(-) p(neut)\\nembeddings\\none-hot vectors\\ndessert was great\\nN⨉d\\n0 0 1 00\\n1 |V|3\\n0 0 1 00\\n1 |V|902\\n0 0 1 00\\n1 |V|524\\n0\\n0\\nE\\nN⨉|V|\\n|V|⨉dE E E matrix\\nshared across words\\nOutput probabilities\\nweights\\nweights\\nsoftmax\\npooled embedding\\nFigure 6.13 Feedforward network sentiment analysis using a pooled embedding of the input words. At each\\ntimestep the network computes a d-dimensional embedding for each context word (by multiplying a one-hot\\nvector by the embedding matrix E), and pools the resulting N embeddings to get a single embedding that\\nrepresents the context window as the layer e.\\ngeneralize better over contexts of similar words, and are far more accurate at word-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='represents the context window as the layer e.\\ngeneralize better over contexts of similar words, and are far more accurate at word-\\nprediction. On the other hand, neural net language models are slower, more com-\\nplex, need vast amounts of energy to train, and are less interpretable than n-gram\\nmodels, so for some smaller tasks an n-gram language model is still the right tool.\\nA feedforward neural language model is a feedforward network that takes as\\ninput at time t a representation of some number of previous words (wt−1,wt−2, etc.)\\nand outputs a probability distribution over possible next words. Thus—like the n-\\ngram LM—the feedforward neural LM approximates the probability of a word given\\nthe entire prior context P(wt |w1:t−1) by approximating based on the N −1 previous\\nwords:\\nP(wt |w1,..., wt−1) ≈P(wt |wt−N+1,..., wt−1) (6.23)\\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to\\nestimate the probability P(wt = i|wt−3,wt−2,wt−1).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='words:\\nP(wt |w1,..., wt−1) ≈P(wt |wt−N+1,..., wt−1) (6.23)\\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to\\nestimate the probability P(wt = i|wt−3,wt−2,wt−1).\\nNeural language models represent words in this prior context by their embed-\\ndings, rather than just by their word identity as used in n-gram language models.\\nUsing embeddings allows neural language models to generalize better to unseen\\ndata. For example, suppose we’ve seen this sentence in training:\\nI have to make sure that the cat gets fed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 17\\nbut have never seen the words “gets fed” after the word “dog”. Our test set has the\\npreﬁx “I forgot to make sure that the dog gets”. What’s the next word? An n-gram\\nlanguage model will predict “fed” after “that the cat gets”, but not after “that the dog\\ngets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will\\nbe able to generalize from the “cat” context to assign a high enough probability to\\n“fed” even after seeing “dog”.\\nh1 h2\\ny1\\nh3 hdh…\\n…\\nU\\nW\\ny34 y|V|\\nembedding layer e 1⨉Nd\\nhidden layer h\\noutput layer y\\nsoftmax\\n…\\n...\\nwt-1wt-2 wtwt-3\\nNd⨉dh\\n1⨉dh\\ndh⨉|V|\\n1⨉|V|\\nInput layer\\none-hot \\nvectors “for” = V35\\n0 0 1 00\\n1 |V|35\\n0 0 1 00\\n1 |V|451\\n0 0 1 00\\n1 |V|992\\n0 0\\n“all” = V992 “the” = V451\\nE\\nN⨉|V|\\nE is shared\\nacross words |V|⨉d\\n…\\np(wt=do|…)p(wt=aardvark|wt-3,wt-2,wt-1) p(wt=zebra|…)p(wt=fish|…)\\n… y42 y35102^^^ ^ ^\\nE E\\nfor all the ?thanksand… …'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='1 |V|992\\n0 0\\n“all” = V992 “the” = V451\\nE\\nN⨉|V|\\nE is shared\\nacross words |V|⨉d\\n…\\np(wt=do|…)p(wt=aardvark|wt-3,wt-2,wt-1) p(wt=zebra|…)p(wt=fish|…)\\n… y42 y35102^^^ ^ ^\\nE E\\nfor all the ?thanksand… …\\nFigure 6.14 Forward inference in a feedforward neural language model. At each timestep\\nt the network computes a d-dimensional embedding for each of the N = 3 context tokens (by\\nmultiplying a one-hot vector by the embedding matrix E), and concatenates the three to get\\nthe embedding e. This embedding e is multiplied by weight matrix W and then an activation\\nfunction is applied element-wise to produce the hidden layer h, which is then multiplied by\\nanother weight matrix U. A softmax layer predicts at each output node i the probability that\\nthe next word wt will be vocabulary wordVi. We show the context window sizeN as 3 just to\\nﬁt on the page, but in practice language modeling requires a much longer context.\\nThis prediction task requires an output vector that expresses |V |probabilities:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ﬁt on the page, but in practice language modeling requires a much longer context.\\nThis prediction task requires an output vector that expresses |V |probabilities:\\none probability value for each possible next token. We might have a vocabulary\\nbetween 60,000 and 300,000 tokens, so the output vector for the task of language\\nmodeling is much longer than 3. Another difference for language modeling is that\\ninstead of pooling the embeddings of the N input tokens to create a single embed-\\nding, we concatenate the inputs into one very long input vector. To predict the next\\ntoken, it helps to know each of the preceding tokens and what order they were in.\\nFig. 6.14 shows the language modeling task, sketched with a very short context\\nwindow of N = 3 just to ﬁt on the page. These 3 embedding vectors are concatenated\\nto produce e, the embedding layer. This is multiplied by a weight matrix W to pro-\\nduce a hidden layer, and another weight matrix U to produce an output layer whose'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='to produce e, the embedding layer. This is multiplied by a weight matrix W to pro-\\nduce a hidden layer, and another weight matrix U to produce an output layer whose\\nsoftmax gives a probability distribution over words. For example y42, the value of\\noutput node 42, is the probability of the next wordwt being V42, the vocabulary word\\nwith index 42 (which is the word ‘ﬁsh’ in our example).\\nThe equations for a simple feedforward neural language model with a window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='18 CHAPTER 6 • N EURAL NETWORKS\\nsize of 3, given one-hot input vectors for each input context word, are:\\ne = [Ext−3;Ext−2;Ext−1]\\nh = σ(We+b)\\nz = Uh\\nˆy = softmax(z) (6.24)\\nNote that we we use semicolons to mean concatenation of vectors, so we form the\\nembedding layer e by concatenating the 3 embeddings for the three context vectors.\\nWe’ll return to this idea of using neural networks to do language modeling in\\nChapter 7 and Chapter 8 when we introduce transformer language models.\\n6.6 Training Neural Nets\\nA feedforward neural net is an instance of supervised machine learning in which we\\nknow the correct output y for each observation x. What the system produces, via\\nEq. 6.13, is ˆy, the system’s estimate of the truey. The goal of the training procedure\\nis to learn parameters W[i] and b[i] for each layer i that make ˆy for each training\\nobservation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='observation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4\\nfor logistic regression, so the reader should be comfortable with that chapter before\\nproceeding. We’ll explore the algorithm on simple generic networks rather than\\nnetworks designed for sentiment or language modeling.\\nFirst, we’ll need a loss function that models the distance between the system\\noutput and the gold output, and it’s common to use the loss function used for logistic\\nregression, the cross-entropy loss.\\nSecond, to ﬁnd the parameters that minimize this loss function, we’ll use the\\ngradient descent optimization algorithm introduced in Chapter 4.\\nThird, gradient descent requires knowing the gradient of the loss function, the\\nvector that contains the partial derivative of the loss function with respect to each\\nof the parameters. In logistic regression, for each observation we could directly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='vector that contains the partial derivative of the loss function with respect to each\\nof the parameters. In logistic regression, for each observation we could directly\\ncompute the derivative of the loss function with respect to an individual w or b. But\\nfor neural networks, with millions of parameters in many layers, it’s much harder to\\nsee how to compute the partial derivative of some weight in layer 1 when the loss\\nis attached to some much later layer. How do we partial out the loss over all those\\nintermediate layers? The answer is the algorithm called error backpropagation or\\nbackward differentiation.\\n6.6.1 Loss function\\nThe cross-entropy loss that is used in neural networks is the same one we saw forcross-entropy\\nloss\\nlogistic regression. If the neural network is being used as a binary classiﬁer, with\\nthe sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss\\nwe saw in Eq. ??:\\nLCE (ˆy,y) =−log p(y|x) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.25)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss\\nwe saw in Eq. ??:\\nLCE (ˆy,y) =−log p(y|x) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.25)\\nIf we are using the network to classify into 3 or more classes, the loss function is\\nexactly the same as the loss for multinomial regression that we saw in Chapter 4 on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 19\\npage ??. Let’s brieﬂy summarize the explanation here for convenience. First, when\\nwe have more than 2 classes we’ll need to represent both y and ˆy as vectors. Let’s\\nassume we’re doing hard classiﬁcation , where only one class is the correct one.\\nThe true label y is then a vector with K elements, each corresponding to a class,\\nwith yc = 1 if the correct class is c, with all other elements of y being 0. Recall that\\na vector like this, with one value equal to 1 and the rest 0, is called aone-hot vector.\\nAnd our classiﬁer will produce an estimate vector with K elements ˆy, each element\\nˆyk of which represents the estimated probability p(yk = 1|x).\\nThe loss function for a single example x is the negative sum of the logs of the K\\noutput classes, each weighted by their probability yk:\\nLCE (ˆy,y) =−\\nK∑\\nk=1\\nyk log ˆyk (6.26)\\nWe can simplify this equation further; let’s ﬁrst rewrite the equation using the func-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='output classes, each weighted by their probability yk:\\nLCE (ˆy,y) =−\\nK∑\\nk=1\\nyk log ˆyk (6.26)\\nWe can simplify this equation further; let’s ﬁrst rewrite the equation using the func-\\ntion 1 {}which evaluates to 1 if the condition in the brackets is true and to 0 oth-\\nerwise. This makes it more obvious that the terms in the sum in Eq. 6.26 will be 0\\nexcept for the term corresponding to the true class for which yk = 1:\\nLCE (ˆy,y) = −\\nK∑\\nk=1\\n1 {yk = 1}log ˆyk\\nIn other words, the cross-entropy loss is simply the negative log of the output proba-\\nbility corresponding to the correct class, and we therefore also call this the negative\\nlog likelihood loss:negative log\\nlikelihood loss\\nLCE (ˆy,y) = −log ˆyc (where c is the correct class) (6.27)\\nPlugging in the softmax formula from Eq. 6.9, and with K the number of classes:\\nLCE (ˆy,y) = −log exp(zc)∑K\\nj=1 exp(zj)\\n(where c is the correct class) (6.28)\\nLet’s think about the negative log probability as a loss function. A perfect clas-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='LCE (ˆy,y) = −log exp(zc)∑K\\nj=1 exp(zj)\\n(where c is the correct class) (6.28)\\nLet’s think about the negative log probability as a loss function. A perfect clas-\\nsiﬁer would assign the correct class i probability 1 and all the incorrect classes prob-\\nability 0. That means the higher p(ˆyi) (the closer it is to 1), the better the classiﬁer;\\np(ˆyi) is (the closer it is to 0), the worse the classiﬁer. The negative log of this prob-\\nability is a beautiful loss metric since it goes from 0 (negative log of 1, no loss)\\nto inﬁnity (negative log of 0, inﬁnite loss). This loss function also insures that as\\nprobability of the correct answer is maximized, the probability of all the incorrect\\nanswers is minimized; since they all sum to one, any increase in the probability of\\nthe correct answer is coming at the expense of the incorrect answers.\\nThe number K of classes of the output vector ˆy can be small or large. Perhaps'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the correct answer is coming at the expense of the incorrect answers.\\nThe number K of classes of the output vector ˆy can be small or large. Perhaps\\nour task is 3-way sentiment, and then the classes might be positive, negative, and\\nneutral. Or if our task is deciding the part of speech of a word (i.e., whether it is a\\nnoun or verb or adjective, etc.), thenK is set of possible parts of speech in our tagset\\n(of which there are 17 in the tagset we will deﬁne in Chapter 17). And if our task\\nis language modeling, and our classiﬁer is trying to predict which word is next, then\\nour set of classes is the set of words, which might be 50,000 or 100,000.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='20 CHAPTER 6 • N EURAL NETWORKS\\n6.6.2 Computing the Gradient\\nHow do we compute the gradient of this loss function? Computing the gradient\\nrequires the partial derivative of the loss function with respect to each parameter.\\nFor a network with one weight layer and sigmoid output (which is what logistic\\nregression is), we could simply use the derivative of the loss that we used for logistic\\nregression in Eq. 6.29 (and derived in Section ??):\\n∂LCE (ˆy,y)\\n∂wj\\n= ( ˆy −y)xj\\n= (σ(w ·x+b)−y)xj (6.29)\\nOr for a network with one weight layer and softmax output (=multinomial logistic\\nregression), we could use the derivative of the softmax loss from Eq. ??, shown for\\na particular weight wk and input xi\\n∂LCE(ˆy,y)\\n∂wk,i\\n= −(yk −ˆyk)xi\\n= −(yk −p(yk = 1|x))xi\\n= −\\n(\\nyk − exp(wk ·x+bk)∑K\\nj=1 exp(wj ·x+bj)\\n)\\nxi (6.30)\\nBut these derivatives only give correct updates for one weight layer: the last one!\\nFor deep networks, computing the gradients for each weight is much more complex,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='j=1 exp(wj ·x+bj)\\n)\\nxi (6.30)\\nBut these derivatives only give correct updates for one weight layer: the last one!\\nFor deep networks, computing the gradients for each weight is much more complex,\\nsince we are computing the derivative with respect to weight parameters that appear\\nall the way back in the very early layers of the network, even though the loss is\\ncomputed only at the very end of the network.\\nThe solution to computing this gradient is an algorithm called error backprop-\\nagation or backprop (Rumelhart et al., 1986). While backprop was invented spe-error back-\\npropagation\\ncially for neural networks, it turns out to be the same as a more general procedure\\ncalled backward differentiation , which depends on the notion of computation\\ngraphs. Let’s see how that works in the next subsection.\\n6.6.3 Computation Graphs\\nA computation graph is a representation of the process of computing a mathematical\\nexpression, in which the computation is broken down into separate operations, each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6.3 Computation Graphs\\nA computation graph is a representation of the process of computing a mathematical\\nexpression, in which the computation is broken down into separate operations, each\\nof which is modeled as a node in a graph.\\nConsider computing the function L(a,b,c) =c(a +2b). If we make each of the\\ncomponent addition and multiplication operations explicit, and add names (d and e)\\nfor the intermediate outputs, the resulting series of computations is:\\nd = 2 ∗b\\ne = a +d\\nL = c ∗e\\nWe can now represent this as a graph, with nodes for each operation, and di-\\nrected edges showing the outputs from each operation as the inputs to the next, as\\nin Fig. 6.15. The simplest use of computation graphs is to compute the value of\\nthe function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,\\nb = 1, c = −2, and we’ve shown the result of the forward pass to compute the re-\\nsult L(3,1,−2) =−10. In the forward pass of a computation graph, we apply each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 21\\noperation left to right, passing the outputs of each computation as the input to the\\nnext node.\\ne=a+d\\nd = 2b L=ce\\na=3\\nb=1\\nc=-2\\ne=5d=2\\nL=-10\\nforward pass\\na\\nb\\nc\\nFigure 6.15 Computation graph for the functionL(a,b,c) =c(a+2b), with values for input\\nnodes a = 3, b = 1, c = −2, showing the forward pass computation of L.\\n6.6.4 Backward differentiation on computation graphs\\nThe importance of the computation graph comes from the backward pass, which\\nis used to compute the derivatives that we’ll need for the weight update. In this\\nexample our goal is to compute the derivative of the output function L with respect\\nto each of the input variables, i.e., ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c . The derivative ∂L\\n∂a tells us how\\nmuch a small change in a affects L.\\nBackwards differentiation makes use of the chain rule in calculus, so let’s re-chain rule\\nmind ourselves of that. Suppose we are computing the derivative of a composite'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Backwards differentiation makes use of the chain rule in calculus, so let’s re-chain rule\\nmind ourselves of that. Suppose we are computing the derivative of a composite\\nfunction f (x) =u(v(x)). The derivative of f (x) is the derivative ofu(x) with respect\\nto v(x) times the derivative of v(x) with respect to x:\\nd f\\ndx = du\\ndv ·dv\\ndx (6.31)\\nThe chain rule extends to more than two functions. If computing the derivative of a\\ncomposite function f (x) =u(v(w(x))), the derivative of f (x) is:\\nd f\\ndx = du\\ndv ·dv\\ndw ·dw\\ndx (6.32)\\nThe intuition of backward differentiation is to pass gradients back from the ﬁnal\\nnode to all the nodes in the graph. Fig. 6.16 shows part of the backward computation\\nat one node e. Each node takes an upstream gradient that is passed in from its parent\\nnode to the right, and for each of its inputs computes a local gradient (the gradient\\nof its output with respect to its input), and uses the chain rule to multiply these two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='node to the right, and for each of its inputs computes a local gradient (the gradient\\nof its output with respect to its input), and uses the chain rule to multiply these two\\nto compute a downstream gradient to be passed on to the next earlier node.\\nLet’s now compute the 3 derivatives we need. Since in the computation graph\\nL = ce, we can directly compute the derivative ∂L\\n∂c :\\n∂L\\n∂c = e (6.33)\\nFor the other two, we’ll need to use the chain rule:\\n∂L\\n∂a = ∂L\\n∂e\\n∂e\\n∂a\\n∂L\\n∂b = ∂L\\n∂e\\n∂e\\n∂d\\n∂d\\n∂b (6.34)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='22 CHAPTER 6 • N EURAL NETWORKS\\ned L\\ned\\n∂L\\n∂d\\n∂L\\n∂e= ∂e\\n∂d\\n∂L\\n∂e\\n∂e\\n∂d\\nupstream\\n gradientdownstream\\n gradient\\nlocal\\n gradient\\nFigure 6.16 Each node (like e here) takes an upstream gradient, multiplies it by the local\\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\\na downstream gradient to be passed on to a prior node. A node may have multiple local\\ngradients if it has multiple inputs.\\nEq. 6.34 and Eq. 6.33 thus require ﬁve intermediate derivatives: ∂L\\n∂e , ∂L\\n∂c , ∂e\\n∂a , ∂e\\n∂d , and\\n∂d\\n∂b , which are as follows (making use of the fact that the derivative of a sum is the\\nsum of the derivatives):\\nL = ce : ∂L\\n∂e = c, ∂L\\n∂c = e\\ne = a +d : ∂e\\n∂a = 1, ∂e\\n∂d = 1\\nd = 2b : ∂d\\n∂b = 2\\nIn the backward pass, we compute each of these partials along each edge of the\\ngraph from right to left, using the chain rule just as we did above. Thus we begin by\\ncomputing the downstream gradients from nodeL, which are ∂L\\n∂e and ∂L\\n∂c . For node e,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='graph from right to left, using the chain rule just as we did above. Thus we begin by\\ncomputing the downstream gradients from nodeL, which are ∂L\\n∂e and ∂L\\n∂c . For node e,\\nwe then multiply this upstream gradient ∂L\\n∂e by the local gradient (the gradient of the\\noutput with respect to the input), ∂e\\n∂d to get the output we send back to node d: ∂L\\n∂d .\\nAnd so on, until we have annotated the graph all the way to all the input variables.\\nThe forward pass conveniently already will have computed the values of the forward\\nintermediate variables we need (liked and e) to compute these derivatives. Fig. 6.17\\nshows the backward pass.\\ne=d+a\\nd = 2b L=ce\\na=3\\nb=1\\ne=5d=2\\nL=-10\\n \\na\\nb\\nc ∂L=5∂c\\n∂L =-2∂e\\n∂e =1∂d\\n∂d =2∂b\\n∂e =1∂a\\nbackward pass\\nc=-2\\n∂L =-2∂e\\n∂L =5∂c\\n∂L\\n∂d =-2∂e\\n∂d\\n∂L\\n∂e=\\n∂L\\n∂a =-2∂e\\n∂a\\n∂L\\n∂e=\\n∂L\\n∂b =-4∂d\\n∂b\\n∂L\\n∂d=\\nFigure 6.17 Computation graph for the function L(a,b,c) =c(a +2b), showing the backward pass computa-\\ntion of ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 23\\nBackward differentiation for a neural network\\nOf course computation graphs for real neural networks are much more complex.\\nFig. 6.18 shows a sample computation graph for a 2-layer neural network with n0 =\\n2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid\\noutput unit for simplicity. The function that the computation graph is computing is:\\nz[1] = W[1]x+b[1]\\na[1] = ReLU(z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = σ(z[2])\\nˆy = a[2] (6.35)\\nFor the backward pass we’ll also need to compute the loss L. The loss function\\nfor binary sigmoid output from Eq. 6.25 is\\nLCE (ˆy,y) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.36)\\nOur output ˆy = a[2], so we can rephrase this as\\nLCE (a[2],y) = −\\n[\\nyloga[2] +(1 −y)log(1 −a[2])\\n]\\n(6.37)\\nz[2] = \\n+ a[2] = σ\\n \\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1] *\\n*\\n*\\n*\\nx1\\nx2\\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\nw[2]\\n11\\nw[1]\\n11\\nw[1]\\n12\\nw[1]\\n21\\nw[1]\\n22 b[2]\\nw[2]\\n12\\nL (a[2],y)1\\n2\\n1\\n1 1\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content=']\\n(6.37)\\nz[2] = \\n+ a[2] = σ\\n \\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1] *\\n*\\n*\\n*\\nx1\\nx2\\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\nw[2]\\n11\\nw[1]\\n11\\nw[1]\\n12\\nw[1]\\n21\\nw[1]\\n22 b[2]\\nw[2]\\n12\\nL (a[2],y)1\\n2\\n1\\n1 1\\n22\\nFigure 6.18 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\\nand 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]\\n11 means\\nthat w[1]\\n11 is to be multiplied by x1, and the node z[1] = +means that the value of z[1] is computed by summing\\nthe three nodes that feed into it (the two products, and the bias term b[1]\\ni ).\\nThe weights that need updating (those for which we need to know the partial\\nderivative of the loss function) are shown in teal. In order to do the backward pass,\\nwe’ll need to know the derivatives of all the functions in the graph. We already saw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='derivative of the loss function) are shown in teal. In order to do the backward pass,\\nwe’ll need to know the derivatives of all the functions in the graph. We already saw\\nin Section ?? the derivative of the sigmoid σ:\\ndσ(z)\\ndz = σ(z)(1 −σ(z)) (6.38)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='24 CHAPTER 6 • N EURAL NETWORKS\\nWe’ll also need the derivatives of each of the other activation functions. The\\nderivative of tanh is:\\nd tanh(z)\\ndz = 1 −tanh2(z) (6.39)\\nThe derivative of the ReLU is2\\nd ReLU(z)\\ndz =\\n{0 f or z < 0\\n1 f or z ≥0 (6.40)\\nWe’ll give the start of the computation, computing the derivative of the loss function\\nL with respect to z, or ∂L\\n∂z (and leaving the rest of the computation as an exercise for\\nthe reader). By the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z (6.41)\\nSo let’s ﬁrst compute ∂L\\n∂a[2] , taking the derivative of Eq. 6.37, repeated here:\\nLCE (a[2],y) = −\\n[\\nyloga[2] +(1 −y)log(1 −a[2])\\n]\\n∂L\\n∂a[2] = −\\n((\\ny∂ log(a[2])\\n∂a[2]\\n)\\n+(1 −y)∂ log(1 −a[2])\\n∂a[2]\\n)\\n= −\\n((\\ny 1\\na[2]\\n)\\n+(1 −y) 1\\n1 −a[2] (−1)\\n)\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\n(6.42)\\nNext, by the derivative of the sigmoid:\\n∂a[2]\\n∂z = a[2](1 −a[2])\\nFinally, we can use the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\na[2](1 −a[2])\\n= a[2] −y (6.43)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content=')\\n(6.42)\\nNext, by the derivative of the sigmoid:\\n∂a[2]\\n∂z = a[2](1 −a[2])\\nFinally, we can use the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\na[2](1 −a[2])\\n= a[2] −y (6.43)\\nContinuing the backward computation of the gradients (next by passing the gra-\\ndients over b[2]\\n1 and the two product nodes, and so on, back to all the teal nodes), is\\nleft as an exercise for the reader.\\n6.6.5 More details on learning\\nOptimization in neural networks is a non-convex optimization problem, more com-\\nplex than for logistic regression, and for that and other reasons there are many best\\npractices for successful learning.\\n2 The derivative is actually undeﬁned at the point z = 0, but by convention we treat it as 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.7 • S UMMARY 25\\nFor logistic regression we can initialize gradient descent with all the weights and\\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\\nweights with small random numbers. It’s also helpful to normalize the input values\\nto have 0 mean and unit variance.\\nVarious forms of regularization are used to prevent overﬁtting. One of the most\\nimportant is dropout: randomly dropping some units and their connections fromdropout\\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\\neach unit we replace its output with zero with probability p (and renormalize the\\nrest of the outputs from that layer).\\nTuning of hyperparameters is also important. The parameters of a neural net-hyperparameter'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='rest of the outputs from that layer).\\nTuning of hyperparameters is also important. The parameters of a neural net-hyperparameter\\nwork are the weights W and biases b; those are learned by gradient descent. The\\nhyperparameters are things that are chosen by the algorithm designer; optimal val-\\nues are tuned on a devset rather than by gradient descent learning on the training\\nset. Hyperparameters include the learning rate η, the mini-batch size, the model\\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\\nof activation functions), how to regularize, and so on. Gradient descent itself also\\nhas many architectural variants such as Adam (Kingma and Ba, 2015).\\nFinally, most modern neural networks are built using computation graph for-\\nmalisms that make it easy and natural to do gradient computation and parallelization\\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='malisms that make it easy and natural to do gradient computation and parallelization\\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\\nreader should consult a neural network textbook for further details; some sugges-\\ntions are at the end of the chapter.\\n6.7 Summary\\n• Neural networks are built out ofneural units, originally inspired by biological\\nneurons but now simply an abstract computational device.\\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\\nthen applies a non-linear activation function like sigmoid, tanh, or rectiﬁed\\nlinear unit.\\n• In a fully-connected, feedforward network, each unit in layer i is connected\\nto each unit in layer i +1, and there are no cycles.\\n• The power of neural networks comes from the ability of early layers to learn\\nrepresentations that can be utilized by later layers in the network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='• The power of neural networks comes from the ability of early layers to learn\\nrepresentations that can be utilized by later layers in the network.\\n• Neural networks are trained by optimization algorithms like gradient de-\\nscent.\\n• Error backpropagation, backward differentiation on a computation graph,\\nis used to compute the gradients of the loss function for a network.\\n• Neural language models use a neural network as a probabilistic classiﬁer, to\\ncompute the probability of the next word given the previous n words.\\n• Neural language models can use pretrained embeddings, or can learn embed-\\ndings from scratch in the process of language modeling.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='26 CHAPTER 6 • N EURAL NETWORKS\\nHistorical Notes\\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\\nloch and Pitts, 1943), a simpliﬁed model of the biological neuron as a kind of com-\\nputing element that could be described in terms of propositional logic. By the late\\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\\nBernard Widrow at Stanford) developed research into neural networks; this phase\\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\\nThe ﬁeld of neural networks declined after it was shown that a single perceptron\\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\\nWhile some small amount of work continued during the next two decades, a major\\nrevival for the ﬁeld didn’t come until the 1980s, when practical tools for building'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='While some small amount of work continued during the next two decades, a major\\nrevival for the ﬁeld didn’t come until the 1980s, when practical tools for building\\ndeeper networks like error backpropagation became widespread (Rumelhart et al.,\\n1986). During the 1980s a wide variety of neural network and related architec-\\ntures were developed, particularly for applications in psychology and cognitive sci-\\nence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\\nand McClelland 1986a, Elman 1990), for which the term connectionist or paral-connectionist\\nlel distributed processing was often used (Feldman and Ballard 1982, Smolensky\\n1988). Many of the principles and techniques developed in this period are foun-\\ndational to modern work, including the ideas of distributed representations (Hinton,\\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\\n(Smolensky, 1990).\\nBy the 1990s larger neural networks began to be applied to many practical lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\\n(Smolensky, 1990).\\nBy the 1990s larger neural networks began to be applied to many practical lan-\\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\\nspeech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\\nin computer hardware and advances in optimization and training techniques made it\\npossible to train even larger and deeper networks, leading to the modern term deep\\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\\nChapter 13 and Chapter 15.\\nThere are a number of excellent books on neural networks, including Goodfellow\\net al. (2016) and Nielsen (2015).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Historical Notes 27\\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-\\nard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-\\nberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,\\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-\\nwar, P. Tucker, V . Vanhoucke, V . Vasudevan, F. Vi´egas,\\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y . Yu,\\nand X. Zheng. 2015. TensorFlow: Large-scale machine\\nlearning on heterogeneous systems. Software available\\nfrom tensorﬂow.org.\\nBengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\nA neural probabilistic language model. JMLR, 3:1137–\\n1155.\\nBengio, Y ., P. Lamblin, D. Popovici, and H. Larochelle.\\n2007. Greedy layer-wise training of deep networks.\\nNeurIPS.\\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\\nence, 14(2):179–211.\\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='NeurIPS.\\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\\nence, 14(2):179–211.\\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-\\nels and their properties. Cognitive Science, 6:205–254.\\nGoodfellow, I., Y . Bengio, and A. Courville. 2016. Deep\\nLearning. MIT Press.\\nHinton, G. E. 1986. Learning distributed representations of\\nconcepts. COGSCI.\\nHinton, G. E., S. Osindero, and Y .-W. Teh. 2006. A fast\\nlearning algorithm for deep belief nets. Neural computa-\\ntion, 18(7):1527–1554.\\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. 2012. Improving neural networks\\nby preventing co-adaptation of feature detectors. ArXiv\\npreprint arXiv:1207.0580.\\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\\noptimization. ICLR 2015.\\nLeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\\nagation applied to handwritten zip code recognition.Neu-\\nral computation, 1(4):541–551.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='LeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\\nagation applied to handwritten zip code recognition.Neu-\\nral computation, 1(4):541–551.\\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\\nof speech perception. Cognitive Psychology, 18:1–86.\\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\\nideas immanent in nervous activity. Bulletin of Mathe-\\nmatical Biophysics, 5:115–133.\\nMinsky, M. and S. Papert. 1969. Perceptrons. MIT Press.\\nMorgan, N. and H. Bourlard. 1990. Continuous speech\\nrecognition using multilayer perceptrons with hidden\\nmarkov models. ICASSP.\\nNielsen, M. A. 2015. Neural networks and Deep learning .\\nDetermination Press USA.\\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n2017. Automatic differentiation in pytorch. NIPS-W.\\nRosenblatt, F. 1958. The perceptron: A probabilistic model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n2017. Automatic differentiation in pytorch. NIPS-W.\\nRosenblatt, F. 1958. The perceptron: A probabilistic model\\nfor information storage and organization in the brain.Psy-\\nchological review, 65(6):386–408.\\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\\nLearning internal representations by error propagation. In\\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\\ntributed Processing, volume 2, 318–362. MIT Press.\\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\\nthe past tense of English verbs. In D. E. Rumelhart and\\nJ. L. McClelland, eds, Parallel Distributed Processing,\\nvolume 2, 216–271. MIT Press.\\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\\nDistributed Processing. MIT Press.\\nRussell, S. and P. Norvig. 2002. Artiﬁcial Intelligence: A\\nModern Approach, 2nd edition. Prentice Hall.\\nSmolensky, P. 1988. On the proper treatment of connection-\\nism. Behavioral and brain sciences, 11(1):1–23.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Modern Approach, 2nd edition. Prentice Hall.\\nSmolensky, P. 1988. On the proper treatment of connection-\\nism. Behavioral and brain sciences, 11(1):1–23.\\nSmolensky, P. 1990. Tensor product variable binding and\\nthe representation of symbolic structures in connectionist\\nsystems. Artiﬁcial intelligence, 46(1-2):159–216.\\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\\nand R. R. Salakhutdinov. 2014. Dropout: a simple\\nway to prevent neural networks from overﬁtting. JMLR,\\n15(1):1929–1958.\\nWidrow, B. and M. E. Hoff. 1960. Adaptive switching cir-\\ncuits. IRE WESCON Convention Record, volume 4.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62698be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\n",
      "Object Detection with Deep Learning: A Review\n",
      "Zhong-Qiu Zhao, Member, IEEE, Peng Zheng,\n",
      "Shou-tao Xu, and Xindong Wu, Fellow, IEEE\n",
      "Abstract—Due to object detection’s close relationship with\n",
      "video analysis and image understanding, it has attracted much\n",
      "research attention in recent years. Traditional object detection\n",
      "methods are built on handcrafted features and shallow trainable\n",
      "architectures. Their performance easily stagnates by constructing\n",
      "complex ensembles which combine multiple low-level image\n",
      "features with high-level context from object detectors and scene\n",
      "classiﬁers. With the rapid development in deep learning, more\n",
      "powerful tools, which are able to learn semantic, high-level,\n",
      "deeper features, are introduced to address the problems existing\n",
      "in traditional architectures. These models behave differently\n",
      "in network architecture, training strategy and optimization...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Content: {chunks[0].page_content[:2000]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed81b16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed67845",
   "metadata": {},
   "source": [
    "embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf33d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab2907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode([\"Nevin is good\", \"kevin is goopd\"], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73dea993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.96324888e-02, -6.83456808e-02,  3.28811840e-03, -4.77464311e-02,\n",
       "       -7.04665259e-02, -1.35691576e-02,  7.44805261e-02,  4.28414680e-02,\n",
       "        1.87391154e-02,  3.16361487e-02, -2.51439940e-02, -4.87189814e-02,\n",
       "       -2.84792781e-02,  6.90285340e-02, -2.67936382e-02, -3.48505639e-02,\n",
       "        6.01727590e-02,  4.94053066e-02,  3.09734065e-02, -1.33773446e-01,\n",
       "       -1.23999625e-01,  7.74960145e-02, -5.40082902e-02, -6.93457946e-02,\n",
       "        7.34248664e-03, -8.94355252e-02,  2.93188952e-02, -1.29067758e-02,\n",
       "       -4.13631368e-03, -6.51442260e-02,  2.11257748e-02, -7.71031305e-02,\n",
       "       -5.21254055e-02, -3.25706936e-02, -3.45337205e-02,  5.81358420e-03,\n",
       "       -5.67268655e-02,  2.37448327e-02, -3.98161747e-02,  3.43041196e-02,\n",
       "       -1.34513564e-02, -1.29241534e-02, -1.50455674e-02, -6.27949163e-02,\n",
       "        1.21100955e-02, -4.35552374e-02, -9.05472487e-02, -3.07407975e-02,\n",
       "        1.27217263e-01,  1.92699768e-03, -8.20147023e-02,  4.48161736e-02,\n",
       "       -8.78682919e-03,  2.33947448e-02,  7.73317814e-02,  1.07895926e-01,\n",
       "       -1.41719580e-02, -5.51442355e-02,  1.01986807e-02, -9.21968836e-03,\n",
       "        4.54504602e-02, -4.65145782e-02, -3.84254716e-02, -7.45524466e-02,\n",
       "        2.05779565e-03,  1.75685564e-03, -7.82306399e-03,  7.06034973e-02,\n",
       "       -2.95099206e-02,  7.76618347e-02,  1.07483886e-01, -1.14894686e-02,\n",
       "        1.54279927e-02, -1.94813125e-02,  9.19844431e-04,  9.68719125e-02,\n",
       "        1.60256717e-02,  1.60589372e-03,  3.31703983e-02, -1.21197738e-02,\n",
       "        7.09874779e-02, -8.38794932e-03, -2.72293370e-02, -2.93539055e-02,\n",
       "        1.73263401e-02,  1.10659031e-02,  3.38978739e-03, -1.65318381e-02,\n",
       "        2.48569110e-03,  6.86425343e-02,  8.40510149e-03,  3.66794877e-02,\n",
       "       -3.31195220e-02, -8.06214213e-02,  1.55049860e-02, -2.67348047e-02,\n",
       "       -4.25092839e-02, -8.41228850e-03, -1.33176416e-01,  9.64418203e-02,\n",
       "       -6.01377431e-03,  1.24447487e-01,  3.50900032e-02, -3.65840532e-02,\n",
       "        1.01254890e-02, -3.70532200e-02,  1.05533622e-01, -9.78080463e-03,\n",
       "        3.17067057e-02,  6.48807585e-02,  3.07477638e-02,  4.89922576e-02,\n",
       "       -9.07453150e-02,  2.43569128e-02,  1.03707835e-02,  7.10794097e-03,\n",
       "        1.94060002e-02,  5.04406542e-02, -9.04162824e-02, -2.64518335e-02,\n",
       "        1.08409058e-02,  4.44223993e-02, -7.78322965e-02,  2.46205777e-02,\n",
       "       -1.17673567e-02,  1.22191850e-02,  2.52318718e-02, -6.52611229e-34,\n",
       "       -5.46118319e-02,  5.15146032e-02, -4.95628780e-03,  2.44665146e-02,\n",
       "        9.94947460e-03,  1.25838518e-02, -5.80669604e-02,  3.82608399e-02,\n",
       "       -1.00466415e-01, -1.07283778e-01,  3.69863352e-03, -4.23570052e-02,\n",
       "       -7.64142051e-02, -1.36957383e-02, -1.09164249e-02,  5.25261648e-02,\n",
       "       -1.21903652e-02, -2.28116885e-02, -8.67083576e-03,  2.43482869e-02,\n",
       "        6.43619290e-03,  6.73208162e-02, -5.28208613e-02, -4.06836979e-02,\n",
       "        1.69847868e-02, -1.26686379e-01,  3.98377003e-03, -1.99645404e-02,\n",
       "       -2.16548797e-02, -3.14920954e-02, -4.17642444e-02,  1.55491065e-02,\n",
       "        2.47045094e-03,  5.68838827e-02,  4.95589189e-02,  6.19563228e-03,\n",
       "       -7.91527480e-02,  4.52229939e-02,  1.27318688e-02, -3.77587751e-02,\n",
       "       -1.32727716e-02,  1.35901317e-01,  4.58695367e-02, -3.35809439e-02,\n",
       "        2.44555122e-04,  5.96556440e-02,  8.32786337e-02, -2.52315737e-02,\n",
       "        7.74638262e-03, -4.47081178e-02,  5.35882115e-02, -4.07953784e-02,\n",
       "       -3.55032682e-02,  6.27780100e-03, -3.88345122e-02, -6.94378093e-03,\n",
       "        2.30861623e-02,  2.97348611e-02,  5.59182428e-02,  2.24463549e-02,\n",
       "        5.06784618e-02,  2.10338365e-02, -3.92979942e-02, -3.92996967e-02,\n",
       "       -1.80528704e-02,  5.25877923e-02, -4.27494347e-02, -2.42687849e-04,\n",
       "        1.58120561e-02, -4.97898348e-02, -3.18306200e-02,  3.08773592e-02,\n",
       "        5.97226210e-02,  3.81967006e-03,  2.90820915e-02,  2.88940426e-02,\n",
       "       -3.40023935e-02, -1.67595595e-02,  9.48053226e-03, -6.56706244e-02,\n",
       "        4.62016184e-03, -5.41444793e-02, -5.20831868e-02,  2.27026157e-02,\n",
       "        6.00505248e-03, -6.89204708e-02, -1.81638282e-02,  7.59332255e-02,\n",
       "       -4.35684016e-03,  8.06319714e-03,  1.08459167e-01, -2.70403177e-03,\n",
       "       -2.56637577e-02, -2.88719982e-02, -7.03357533e-02,  9.44004524e-34,\n",
       "       -2.44050827e-02,  2.87976414e-02, -1.04520479e-02,  1.20128728e-01,\n",
       "        1.66754741e-02, -2.47172848e-03,  3.01996507e-02,  4.00714986e-02,\n",
       "        4.26187553e-02, -5.16178533e-02,  5.22447154e-02,  1.81635208e-02,\n",
       "        4.51621711e-02, -3.17465630e-03, -1.06006290e-03, -4.75992896e-02,\n",
       "       -4.65458399e-03, -3.59760895e-02,  5.39018661e-02, -2.19957419e-02,\n",
       "        7.18176216e-02,  6.98072985e-02, -3.55704650e-02, -4.11428586e-02,\n",
       "        3.22013758e-02,  5.55236898e-02, -2.90508065e-02,  5.99529520e-02,\n",
       "       -1.56984732e-01,  3.82894948e-02, -1.25614286e-04,  1.79805923e-02,\n",
       "       -7.12912064e-03, -6.90856529e-03,  3.32700945e-02,  2.83380710e-02,\n",
       "        8.19120556e-02,  6.32293075e-02,  1.10282078e-02, -1.14662489e-02,\n",
       "        2.16103066e-02,  3.43848392e-02, -2.78768456e-03,  4.50945683e-02,\n",
       "       -5.31493127e-03, -2.02559717e-02,  1.65062174e-02,  7.99193829e-02,\n",
       "       -2.67656129e-02,  8.04292262e-02, -6.94169924e-02,  3.19597945e-02,\n",
       "       -1.18720338e-01,  3.77320684e-02, -4.25671674e-02,  2.96864361e-02,\n",
       "        1.86478738e-02, -4.91658375e-02, -2.04646010e-02,  2.09095143e-02,\n",
       "       -4.67930324e-02,  5.74981794e-02, -1.60858165e-02, -2.71824491e-03,\n",
       "        2.93036364e-03,  3.88201438e-02, -2.09402610e-02,  9.25695002e-02,\n",
       "       -8.49208608e-02, -5.01550920e-02, -1.06276244e-01,  4.74441946e-02,\n",
       "        3.36115658e-02, -5.03741466e-02, -7.76770562e-02, -3.81799266e-02,\n",
       "        7.97798261e-02,  3.45872482e-03, -3.57394805e-04, -2.18715277e-02,\n",
       "       -3.84838320e-02, -5.15355319e-02, -1.55002609e-01,  9.70496535e-02,\n",
       "        1.90918185e-02,  8.06771293e-02,  5.26752286e-02, -9.65958834e-02,\n",
       "        6.76590134e-04, -1.54384719e-02, -1.79745292e-03,  8.08949675e-03,\n",
       "       -1.56767573e-02,  1.51243592e-02,  1.75137930e-02, -1.17602745e-08,\n",
       "        1.12241122e-03,  2.97644679e-02,  2.40739156e-02,  1.81812933e-03,\n",
       "        5.22561781e-02, -2.90286615e-02, -6.71236664e-02, -4.91049439e-02,\n",
       "        5.57169691e-03,  1.31511256e-01,  1.19984284e-01,  4.65689786e-02,\n",
       "       -9.11467895e-02, -9.92635414e-02, -7.95192830e-03,  1.65869854e-02,\n",
       "       -3.61084845e-03,  2.83228904e-02, -4.75851446e-03,  3.72028016e-02,\n",
       "        1.15332035e-02,  6.06905669e-02, -3.66051383e-02,  2.23378721e-03,\n",
       "        2.74269879e-02, -3.10420990e-02,  2.75609158e-02, -2.27829367e-02,\n",
       "        2.54069716e-02, -5.70739806e-03,  2.68477984e-02,  9.32077616e-02,\n",
       "        3.08245607e-02,  1.25662489e-02,  1.06938146e-01, -1.16391078e-01,\n",
       "       -5.98491021e-02,  3.39504448e-03,  4.55032997e-02, -7.35406652e-02,\n",
       "        5.46570346e-02,  7.99081996e-02, -6.62040636e-02, -3.54356617e-02,\n",
       "       -5.11219241e-02, -2.09722687e-02,  4.60224040e-02,  3.52679612e-03,\n",
       "        3.52116721e-03,  7.53697902e-02,  4.06019166e-02, -3.78863956e-03,\n",
       "        8.59653801e-02,  7.67367408e-02,  6.86232224e-02,  8.59906804e-03,\n",
       "       -6.40089111e-03,  4.13481779e-02, -4.16485630e-02,  5.74164018e-02,\n",
       "        1.10499468e-02, -6.30740523e-02, -8.06920137e-03,  1.02567889e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f28fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x11eafee50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b6618",
   "metadata": {},
   "source": [
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730e1cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x11e307cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed91e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='are also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='valuable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='or sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='allows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiﬁcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='capacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiﬁcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a ﬁxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]–[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection, but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overﬁtting of training, lack of large scale\\ntraining data, limited computation power and insigniﬁcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n•The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n•Fast development of high performance parallel computing\\nsystems, such as GPU clusters;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='as ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n•Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n•Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverﬁtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efﬁcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='What prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‘dropout’ regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‘pixel’ can be viewed as a speciﬁc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='linear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='loss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiﬁcation layer. The conv feature maps are produced by\\nconvoluting 3*3 ﬁlter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n•Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n• The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiﬁcation and bounding box regression\\ninto a multi-task leaning manner).\\n• Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image super-resolution\\nreconstruction [54], [55], image classiﬁcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='age retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conﬁdences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at ﬁrst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiﬁcation problem, adopting a uniﬁed\\nframework to achieve ﬁnal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-\\nCNN with a SPP layer). The regression /classiﬁcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario ﬁrstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='works [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the ﬂowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a ﬁxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the ﬁnal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiﬁcation and localization. With pre-trained category-\\nspeciﬁc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classiﬁcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁ-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\n˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='use of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='recognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classiﬁcation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun’s CNN (e.g., max(x, 0) rectifying non-linearities and\\n“dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Cun’s CNN (e.g., max(x, 0) rectifying non-linearities and\\n“dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classiﬁcation and object detection.\\nThis paper is the ﬁrst to show that a CNN can lead to dra-\\n1\\nFig. 3. The ﬂowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiﬁes each region with class-speciﬁc linear SVMs.\\nbounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN ﬁrstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciﬁc ﬁne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n•Due to the existence of FC layers, the CNN requires a\\nﬁxed-size (e.g., 227×227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n•Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region, taking a great deal of time in the testing period.\\n•Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is\\nﬁne-tuned. Then the softmax classiﬁer learned by ﬁne-\\ntuning is replaced by SVMs to ﬁt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n• Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n•Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reﬁne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciﬁc CNN classiﬁers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiﬁcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a ﬁxed-size input. That’s\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]† 56.07 74.41 ±1.0\\nLLC [18]† 57.66 76.95 ±0.4\\nFK [19]† 61.69 77.78 ±0.6\\nDeCAF [13] - 86.91 ±0.7\\nZeiler & Fergus [4] 75.90‡ 86.5±0.5\\nOquab et al. [34] 77.7 -\\nChatﬁeld et al. [6] 82.42 88.54±0.3\\nours 82.44 93.42 ±0.5\\nTable 8: Classiﬁcation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). †numbers reported\\nby [27]. ‡our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brieﬂy review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN ﬁrst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='method [7]. R-CNN ﬁrst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a ﬁxed size (227 ×227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classiﬁer is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\ndeﬁne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the “fast” mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 ×1, 2×2, 3×3, 6×6, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 ×50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='d (256 ×50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classiﬁer for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classiﬁer\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='non-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns ∈ S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically ﬁnd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s ∈ S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 ×224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, ﬁne-tuning a network with log loss, training SVMs,\\nand ﬁnally ﬁtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the ﬁne-tuning al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tures, ﬁne-tuning a network with log loss, training SVMs,\\nand ﬁnally ﬁtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the ﬁne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (ﬁxed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that ﬁxes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it’s comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network ﬁrst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a ﬁxed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that ﬁnally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all “background” class and\\nanother layer that outputs four real-valued numbers for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ers: one that produces softmax probability estimates over\\nK object classes plus a catch-all “background” class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes reﬁned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a ﬁxed spatial extent of H × W (e.g., 7 × 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\ndeﬁned by a four-tuple (r, c, h, w ) that speciﬁes its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a ﬁxed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h × w RoI win-\\ndow into an H × W grid of sub-windows of approximate\\nsize h/H × w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets ['),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with ﬁve max pooling layers and between ﬁve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is conﬁgured by setting H and W to be\\ncompatible with the net’s ﬁrst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network’s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiﬁ-\\ncation) are replaced with the two sibling layers described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='H = W = 7 for VGG16).\\nSecond, the network’s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiﬁ-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speciﬁc bounding-box regressors).\\nThird, the network is modiﬁed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let’s elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inefﬁcient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inefﬁciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN and SPPnet networks are trained. The inefﬁciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe ﬁnal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 ×(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='also improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network ﬁne-tuning, SVM training and bounding-\\nbox regressor ﬁtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiﬁcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nﬁnally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one ‘background’\\nclass) and the other output layer encodes reﬁned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train\\nclassiﬁcation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +λ[u≥1]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =−log pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,··· ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u≥1] is employed to omit all background RoIs. To provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u≥1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to ﬁt\\nbounding-box regressors as below\\nLloc(tu,v) =\\n∑\\ni∈x,y,w,h\\nsmoothL1 (tu\\ni −vi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|−0.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefﬁcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at ﬁrst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='where R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efﬁciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciﬁc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers—a box-regression layer ( reg)\\nand a box-classiﬁcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nﬁeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and a box-classiﬁcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nﬁeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n×nconv layer followed by two sibling 1 ×1 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n×nconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW×H(typically ∼2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='translation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) ×800-dimensional output layer,\\nwhereas our method requires a (4+2)×9-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of overﬁtting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these deﬁnitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is deﬁned as:\\nL({pi},{ti}) = 1\\nNcls\\n∑\\ni\\nLcls (pi,p∗\\ni ) +λ 1\\nNreg\\n∑\\ni\\np∗\\ni Lreg(ti,t∗\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn×n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiﬁcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n×n conv layer followed by two sibling 1 ×1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n×n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n∑\\ni\\nLcls(pi,p∗'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n∑\\ni\\nLcls(pi,p∗\\ni) +λ 1\\nNreg\\n∑\\ni\\np∗\\niLreg(ti,t∗\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p∗\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t∗\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='With the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.\\nRecent state-of-the-art image classiﬁcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it’s\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll´ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='intensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signiﬁcant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object’s scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this ﬁgure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In this ﬁgure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k×k ﬁrstly and a position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k×k ﬁrstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiﬁcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='set of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 ×1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3×3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the ﬁnal feature map is\\ngenerated. This process is iterated until the ﬁnest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiﬁcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m×m mask to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m×m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='It affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='other tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a ﬂexible and efﬁcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiﬁcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‘stuff’ (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive ﬁelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiﬁers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='support regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiﬁcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiﬁed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='summing different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiﬁers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiﬁer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Traditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiﬁcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nﬁrstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /Classiﬁcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='eral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classiﬁcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We ﬁrstly reviews some pioneer CNN\\nmodels, and then focus on two signiﬁcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed\\nloss was introduced to bias both localization and conﬁdences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the ﬁnal layer.\\nYoo et al. adopted an iterative classiﬁcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\nﬁnding a path from a ﬁxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a ﬁxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifﬁculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conﬁdences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S×S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOUtruth\\npred , which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) ≥0) and\\nshows conﬁdences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ∗IOUtruth\\npred ∗Pr(Classi|Object)\\n= Pr(Classi) ∗IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='= Pr(Classi) ∗IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nλcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[\\n(xi −ˆxi)2 + (yi −ˆyi)2]\\n+λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[(√wi −\\n√\\nˆwi)2 + (\\n√\\nhi −\\n√\\nˆhi\\n)2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n(\\nCi −ˆCi\\n)2\\n+λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 noobj\\nij\\n(\\nCi −ˆCi\\n)2\\n+\\nS2\\n∑\\ni=0\\n1 obj\\ni\\n∑\\nc∈classes\\n(pi(c) −ˆpi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents conﬁdence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,\\nwhen the predictor is ‘responsible’ for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 ×1 reduction layers followed by 3 ×3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliﬁed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='makes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconﬁgurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciﬁc feature map, instead of\\nﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conﬁdences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conﬁdence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reﬁned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Integrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300×300) runs\\nat 59 FPS, which is more accurate and efﬁcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection results are obtained by conducting NMS on multi-scale reﬁned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciﬁc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiﬁcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n•If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n• With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n•Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n•Apart from basic models, there are still many other factors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='important for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n•Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nﬁed classiﬁcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n•As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and ﬁne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n•By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='•By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n• Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classﬁcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classﬁcation based approaches.\\n• Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n•Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='objects and surroundings (GBD-Net and multi-path).\\n•Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n•The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiﬁers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classiﬁcation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classiﬁcation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='FPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conﬁdence+background conﬁdence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conﬁdence+background conﬁdence\\n* ‘+’ denotes that corresponding techniques are employed while ‘-’ denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* ‘07’: VOC2007 trainval, ‘07+12’: union of VOC2007 and VOC2012 trainval, ‘07+12+COCO’: trained on COCO trainval35k at ﬁrst and then ﬁne-tuned on 07+12. The S in ION ‘07+12+S’ denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='HyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* ‘07++12’: union of VOC2007 trainval and test and VOC2012 trainval. ‘07++12+COCO’: trained on COCO trainval35k at ﬁrst then ﬁne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‘SS’ which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n• By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniﬁed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It’s also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='PV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n•It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n•It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n•Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiﬁed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniﬁed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial. To learn internal represen-\\ntations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive ﬁelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='convolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye ﬁxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='this problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Two standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as below\\nFβ = (1 +β2)Presion ×Recall\\nβ2Presion + Recall (7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH×W\\nH∑\\ni=1\\nW∑\\nj=1\\n⏐⏐⏐ˆS(i,j) = ˆZ(i,j)\\n⏐⏐⏐ (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can ﬁnd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufﬁcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='segmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it’s necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='complementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]–[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conﬁgurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='been proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='into smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefﬁcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wFβ 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wFβ 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wFβ 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wFβ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SOD wFβ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wFβ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiﬁcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniﬁed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a ﬁxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='addressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conﬁguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-ﬁne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='To reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reﬁne the positions of possible\\nfaces. Qin et al. proposed a uniﬁed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='types of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reﬂection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='NPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiﬁcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='which has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‘plain’ features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand ﬁne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='al. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiﬁers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and\\nsimpliﬁcation is of signiﬁcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‘Person (clear identiﬁcations)’,\\n‘Person? (unclear identiﬁcations)’ and ‘People (large group of\\nindividuals)’, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10−2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The ﬁrst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='candidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciﬁc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='application, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n•Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneﬁcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='jects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n•Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='selection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n•Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiﬁers at latter stages can\\nhandle more difﬁcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are ﬁxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='when training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n• Unsupervised and weakly supervised learning. It’s\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reﬁne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n•3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='transportation and intelligent surveillance.\\n•3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n• Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='blur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, “Pedestrian detection:\\nAn evaluation of the state of the art,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis.” IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235–245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\\nfast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='fast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, “Realtime multi-person\\n2d pose estimation using part afﬁnity ﬁelds,” in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, “Deepdriving:\\nLearning affordance for direct perception in autonomous driving,” in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object\\ndetection network for autonomous driving,” in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, “Embedded streaming\\ndeep neural networks accelerator with applications,” IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Neural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity\\napproximate convolutional neural networks,”IEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1–12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n“Cost-sensitive learning of deep feature representations from imbal-\\nanced data.” IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1–15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, “Feature extraction with deep\\nneural networks by a generalized discriminant analysis.” IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596–608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic segmentation,”\\nin CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\\nonce: Uniﬁed, real-time object detection,” in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\\ntime object detection with region proposal networks,” in NIPS, 2015,\\npp. 91–99.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\\ndetection,” in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, “An extended set of haar-like features for\\nrapid object detection,” in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,”Machine Learning,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y . Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[23] Y . Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627–1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, “The pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),” 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol.\\n521, no. 7553, pp. 436–444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, “Predicting eye ﬁxations\\nusing convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, “Face detection with the faster r-cnn,”\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, “Joint cascade face\\ndetection and alignment,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, “Supervised transformer network\\nfor efﬁcient face detection,” in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, “A real-time\\npedestrian detector using deep learning for human-aware navigation,”\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, “Exploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiﬁers,” in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, “A survey of deep learning methods and\\nsoftware tools for image classiﬁcation and object detection,” Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,”The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,”The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal\\nrepresentation by back-propagation of errors,” Nature, vol. 323, no.\\n323, pp. 533–536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality\\nof data with neural networks,” Sci., vol. 313, pp. 504–507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., “Deep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,” IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82–97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, “Binary coding of speech spectrograms using a deep auto-\\nencoder,” in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., “Phone recognition with\\nthe mean-covariance restricted boltzmann machine,” in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, “Improving neural networks by preventing co-\\nadaptation of feature detectors,” arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks\\nfor large-scale image recognition,” arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nboltzmann machines,” in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , “Weakly supervised\\nobject recognition with convolutional neural networks,” in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.\\n[51] F. M. Wadley, “Probit analysis: a statistical treatment of the sigmoid\\nresponse curve,” Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549–553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , “Learning invariant\\nfeatures through topographic ﬁlter maps,” in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, “Learning convolutional feature hierarchies for visual\\nrecognition,” in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Deconvolu-\\ntional networks,” in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for\\nsemantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive\\nsample learning,” in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, “Neural codes\\nfor image retrieval,” in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n“Deep learning for content-based image retrieval: A comprehensive\\nstudy,” in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. Barofﬁo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, “Deep convolutional neural networks for pedestrian detec-\\ntion,” Signal Process.: Image Commun. , vol. 47, pp. 482–489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, “Subcategory-aware\\nconvolutional neural networks for object proposals and detection,” in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='WACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n“Multimodal deep learning,” in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, “Modeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiﬁ-\\ncation,” in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904–1916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., “R-fcn: Object detection via region-based\\nfully convolutional networks,” in NIPS, 2016, pp. 379–387.\\n[66] T.-Y . Lin, P. Doll ´ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Belongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object\\ndetection using deep neural networks,” in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, “Attentionnet:\\nAggregating weak directions for accurate object detection,” in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, “G-cnn: an iterative grid\\nbased object detector,” in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, “Ssd: Single shot multibox detector,” in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,”\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd:\\nDeconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, “Dsod:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, “Dsod:\\nLearning deeply supervised object detectors from scratch,” in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-\\nencoders,” in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, “Learning invariance\\nthrough imitation,” in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n“Selective search for object recognition,” Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154–171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, “Pedestrian\\ndetection with unsupervised multi-stage feature learning,” in CVPR,\\n2013.\\n[80] P. Kr ¨ahenb¨uhl and V . Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2013.\\n[80] P. Kr ¨ahenb¨uhl and V . Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n“Multiscale combinatorial grouping,” in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll ´ar, “Edge boxes: Locating object proposals\\nfrom edges,” in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, “Deepbox: Learning objectness\\nwith convolutional networks,” in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ´ar, “Learning to\\nreﬁne object segments,” in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,” in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel ´aez, and J. Malik, “Learning rich features\\nfrom rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., “Deepid-net: Deformable deep convolutional\\nneural networks for object detection,” in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, “R-cnn minus r,” arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,”\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S ´anchez, and T. Mensink, “Improving the ﬁsher kernel\\nfor large-scale image classiﬁcation,” in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, “Restructuring of deep neural network\\nacoustic models with singular value decomposition.” in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\\nobject detection with region proposal networks,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\\ning the inception architecture for computer vision,” in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,” in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\\nmulti-task network cascades,” in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, “Fully convolutional instance-\\naware semantic segmentation,” in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial transformer networks,” in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, “Stuffnet: Using stuffto\\nimprove object detection,” in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, “Hypernet: Towards accurate\\nregion proposal generation and joint object detection,” in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, “Curriculum learning\\nof multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, “Multi-stage object\\ndetection with group recursive learning,” arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A uniﬁed multi-scale\\ndeep convolutional neural network for fast object detection,” in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,” in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling\\nwith lstm recurrent neural networks,” in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, “Learning to detect and\\nlocalize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-\\ndirectional cnn for object detection,” in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, “Object detection via a multi-region and\\nsemantic segmentation-aware cnn model,” in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-\\nworks,” IEEE Trans. Signal Process. , vol. 45, pp. 2673–2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll ´ar, “A multipath network for object detection,”\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\\nobject detectors with online hard example mining,” in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\\ndeep model for object detection with long-tail distribution,” in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, “Pvanet:\\nLightweight deep neural networks for real-time object detection,”\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and\\nimproving convolutional neural networks via concatenated rectiﬁed\\nlinear units,” in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\\ndetection,” in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll ´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[119] P. O. Pinheiro, R. Collobert, and P. Doll ´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n“The pascal visual object classes challenge 2012 (voc2012) results\\n(2012),” in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\\ntional networks,” in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll ´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, “Autocollage,”ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, “Autocollage,”ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, “Real-time salient object\\ndetection with a minimum spanning tree,” in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint crf and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576–588, 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Recognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353–367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989–1005, 2009.\\n[133] S. Xie and Z. Tu, “Holistically-nested edge detection,” in ICCV, 2015.\\n[134] M. K ¨ummerer, L. Theis, and M. Bethge, “Deep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,”\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='arXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency\\ndetection via local estimation and global search,” in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, “Weakly supervised top-down\\nsalient object detection,” arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by\\nmulti-context deep learning,” in CVPR, 2015.\\n[139] C ¸ . Bak, A. Erdem, and E. Erdem, “Two-stream convolutional networks\\nfor dynamic saliency prediction,” arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,”\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Int. J. of Comput. Vision , vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, “Deepsaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919–3930, 2016.\\n[142] Y . Tang and X. Wu, “Saliency detection via combining region-level\\nand pixel-level predictions with cnns,” in ECCV, 2016.\\n[143] G. Li and Y . Yu, “Deep contrast learning for salient object detection,”\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, “Edge preserving and\\nmulti-scale contextual neural network for salient object detection,”\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level\\nnetwork for saliency prediction,” in ICPR, 2016.\\n[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor,\\n“Shallow and deep convolutional networks for saliency prediction,” in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for\\nsaliency detection,” in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, “Deeply-supervised recurrent convolutional\\nneural network for saliency detection,” in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, “Contextual\\nhypergraph modeling for salient object detection,” in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\\ncontrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, “Salient object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='contrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, “Salient object\\ndetection: A discriminative regional feature integration approach,” in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, “Deep saliency with encoded low level\\ndistance map and high level features,” in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n“Non-local deep features for salient object detection,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n“Deeply supervised salient object detection with short connections,”\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,” in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of\\nsalient object segmentation,” in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, “Design and perceptual validation of\\nperformance measures for salient object segmentation,” in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:\\nA benchmark,” IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706–\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, “Face recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,” in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, “Face sketchcphoto synthesis\\nand retrieval using sparse representation,” IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213–1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, “A comprehensive survey\\nto face hallucination,” Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9–30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, “Multiple\\nrepresentations-based face sketch-photo synthesis.”IEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Netw. & Learning Syst. , vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial\\nexpression recognition system using deep network-based data fusion,”\\nIEEE Trans. Cybern. , vol. 48, pp. 103–114, 2018.\\n[166] P. Viola and M. Jones, “Robust real-time face detection,” Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, “Unitbox: An advanced\\nobject detection network,” in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, “Multi-view face detection\\nusing deep convolutional neural networks,” in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “From facial parts responses\\nto face detection: A deep learning approach,” in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, “Face detection through\\nscale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face\\ndetection,” in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, “Face r-cnn,” arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, “Face detection using deep learning: An\\nimproved faster rcnn approach,” arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, “Densebox: Unifying landmark\\nlocalization with end to end object detection,” arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, “face detection with end-to-end\\nintegration of a convnet and a 3d model,” in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, “Joint face detection and\\nalignment using multitask cascaded convolutional networks,” IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Signal Process. Lett. , vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, “Joint training of cascaded cnn for\\nface detection,” in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, “Fddb: A benchmark for face detection\\nin unconstrained settings,” Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A convolutional neural\\nnetwork cascade for face detection,” in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Aggregate channel features for\\nmulti-view face detection,” in IJCB, 2014.\\n[182] N. Marku ˇs, M. Frljak, I. S. Pand ˇzi´c, J. Ahlberg, and R. Forchheimer,\\n“Object detection with pixel intensity comparisons organized in deci-\\nsion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.\\n[184] J. Li and Y . Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, “A fast and accurate unconstrained\\nface detector,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211–223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,”\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, “Hyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,” arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.\\n[190] D. Gavrila and S. Munder, “Multi-cue pedestrian detection and tracking\\nfrom a moving vehicle,” Int. J. of Comput. Vision , vol. 73, pp. 41–59,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, “Jointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiﬁcation,” in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, “Stepwise metric promotion for unsuper-\\nvised video person re-identiﬁcation,” in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, “Cooperative robots to observe\\nmoving targets: Review,” IEEE Trans. Cybern. , vol. 48, pp. 187–198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res. , vol. 32, pp. 1231–1237,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res. , vol. 32, pp. 1231–1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, “Learning complexity-aware\\ncascades for deep pedestrian detection,” in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in CVPR, 2015.\\n[197] P. Doll ´ar, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids\\nfor object detection,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532–1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, “Filtered channel features for\\npedestrian detection,” in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, “Pedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained\\nand-or graph models for object shape detection,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959–972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, “Handling\\nocclusions with franken-classiﬁers,” in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, “Detection and tracking of\\noccluded people,” Int. J. of Comput. Vision, vol. 110, pp. 58–69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, “Is faster r-cnn doing well for\\npedestrian detection?” in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, “Multispectral deep\\nneural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by\\ndeep learning semantic tasks,” in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, “Fused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,” in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, “Pushing\\nthe limits of deep cnns for pedestrian detection,” IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom ´e, L. Bondi, L. Barofﬁo, S. Tubaro, E. Plebani, and D. Pau,\\n“Reduced memory region based deep convolutional neural network\\ndetection,” in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a deeper\\nlook at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='look at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, “Visual-textual\\njoint relevance learning for tag-based social image search,”IEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363–376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, “Ron: Reverse\\nconnection with objectness prior networks for object detection,” in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, “Generative adversarial nets,”\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, “Object\\ndetection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, “Learning detection with diverse\\nproposals,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end\\nmemory networks,” in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, “Real time image saliency for black box\\nclassiﬁers,” in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Craft objects from images,” in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, “Unsupervised learning\\nfrom video to detect foreground objects in single images,” in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, “Weakly supervised object\\nlocalization with latent category learning,” in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n“Training object class detectors with click supervision,” inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy\\ntrade-offs for modern convolutional object detectors,” in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, “Mimicking very efﬁcient network for object\\ndetection,” in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, “Fitnets: Hints for thin deep nets,” Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, “3d object proposals for accurate object class detection,”\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n“Object detection in videos with tubelet proposal networks,” in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='His research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor’s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Artiﬁcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='formation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2025. All\\nrights reserved. Draft of August 24, 2025.\\nCHAPTER\\n6\\nNeural Networks\\n“[M]achines of this character can behave in a very complicated manner when\\nthe number of units is large.”\\nAlan Turing (1948) “Intelligent Machines”, page 6\\nNeural networks are a fundamental computational tool for language process-\\ning, and a very old one. They are called neural because their origins lie in the\\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the\\nbiological neuron as a kind of computing element that could be described in terms\\nof propositional logic. But the modern use in language processing no longer draws\\non these early biological inspirations.\\nInstead, a modern neural network is a network of small computing units, each\\nof which takes a vector of input values and produces a single output value. In this\\nchapter we introduce the neural net applied to classiﬁcation. The architecture we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='of which takes a vector of input values and produces a single output value. In this\\nchapter we introduce the neural net applied to classiﬁcation. The architecture we\\nintroduce is called a feedforward network because the computation proceeds iter-feedforward\\natively from one layer of units to the next. The use of modern neural nets is often\\ncalled deep learning, because modern networks are often deep (have many layers).deep learning\\nNeural networks share much of the same mathematics as logistic regression. But\\nneural networks are a more powerful classiﬁer than logistic regression, and indeed a\\nminimal neural network (technically one with a single ‘hidden layer’) can be shown\\nto learn any function.\\nNeural net classiﬁers are different from logistic regression in another way. With\\nlogistic regression, we applied the regression classiﬁer to many different tasks by\\ndeveloping many rich kinds of feature templates based on domain knowledge. When'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='logistic regression, we applied the regression classiﬁer to many different tasks by\\ndeveloping many rich kinds of feature templates based on domain knowledge. When\\nworking with neural networks, it is more common to avoid most uses of rich hand-\\nderived features, instead building neural networks that take raw tokens as inputs\\nand learn to induce features as part of the process of learning to classify. We saw\\nexamples of this kind of representation learning for embeddings in Chapter 5, and\\nwe’ll see lots of examples once we start studying deep transformers networks. Nets\\nthat are very deep are particularly good at representation learning. For that reason\\ndeep neural nets are the right tool for tasks that offer sufﬁcient data to learn features\\nautomatically.\\nIn this chapter we’ll introduce feedforward networks as classiﬁers, ﬁrst with\\nhand-built features, and then using the embeddings that we studied in Chapter 5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='automatically.\\nIn this chapter we’ll introduce feedforward networks as classiﬁers, ﬁrst with\\nhand-built features, and then using the embeddings that we studied in Chapter 5.\\nIn subsequent chapters we’ll introduce many other kinds of neural models, most\\nimportantly the transformer and attention, (Chapter 8), but also recurrent neural\\nnetworks (Chapter 13) and convolutional neural networks (Chapter 15). And in\\nthe next chapter we’ll introduce the paradigm of neural large language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2 CHAPTER 6 • N EURAL NETWORKS\\n6.1 Units\\nThe building block of a neural network is a single computational unit. A unit takes\\na set of real valued numbers as input, performs some computation on them, and\\nproduces an output.\\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\\ntional term in the sum called a bias term. Given a set of inputs x1...xn, a unit hasbias term\\na set of corresponding weights w1...wn and a bias b, so the weighted sum z can be\\nrepresented as:\\nz = b +\\n∑\\ni\\nwixi (6.1)\\nOften it’s more convenient to express this weighted sum using vector notation; recall\\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thusvector\\nwe’ll talk aboutz in terms of a weight vector w, a scalar bias b, and an input vector\\nx, and we’ll replace the sum with the convenientdot product:\\nz = w ·x+b (6.2)\\nAs deﬁned in Eq. 6.2, z is just a real valued number.\\nFinally, instead of using z, a linear function of x, as the output, neural units'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='z = w ·x+b (6.2)\\nAs deﬁned in Eq. 6.2, z is just a real valued number.\\nFinally, instead of using z, a linear function of x, as the output, neural units\\napply a non-linear function f to z. We will refer to the output of this function as\\nthe activation value for the unit, a. Since we are just modeling a single unit, theactivation\\nactivation for the node is in fact the ﬁnal output of the network, which we’ll generally\\ncall y. So the value y is deﬁned as:\\ny = a = f (z)\\nWe’ll discuss three popular non-linear functionsf below (the sigmoid, the tanh, and\\nthe rectiﬁed linear unit or ReLU) but it’s pedagogically convenient to start with the\\nsigmoid function since we saw it in Chapter 4:sigmoid\\ny = σ(z) = 1\\n1 +e−z (6.3)\\nThe sigmoid (shown in Fig. 6.1) has a number of advantages; it maps the output\\ninto the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\\ndifferentiable, which as we saw in Section ?? will be handy for learning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='into the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\\ndifferentiable, which as we saw in Section ?? will be handy for learning.\\nFigure 6.1 The sigmoid function takes a real value and maps it to the range (0,1). It is\\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\\nSubstituting Eq. 6.2 into Eq. 6.3 gives us the output of a neural unit:\\ny = σ(w ·x+b) = 1\\n1 +exp(−(w ·x+b)) (6.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.1 • U NITS 3\\nFig. 6.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit\\ntakes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each\\nvalue by a weight (w1, w2, and w3, respectively), adds them to a bias termb, and then\\npasses the resulting sum through a sigmoid function to result in a number between 0\\nand 1.\\nx1\\nx2\\nx3\\ny\\nw1\\nw2\\nw3\\n∑\\nb\\nσ\\n+1\\nz a\\nFigure 6.2 A neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a\\nweight for an input clamped at +1) and producing an output y. We include some convenient\\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\\nthis case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to\\nmean the ﬁnal output of the entire network, leaving a as the activation of an individual node.\\nLet’s walk through an example just to get an intuition. Let’s suppose we have a\\nunit with the following weight vector and bias:\\nw = [0.2,0.3,0.9]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Let’s walk through an example just to get an intuition. Let’s suppose we have a\\nunit with the following weight vector and bias:\\nw = [0.2,0.3,0.9]\\nb = 0.5\\nWhat would this unit do with the following input vector:\\nx = [0.5,0.6,0.1]\\nThe resulting output y would be:\\ny = σ(w ·x+b) = 1\\n1 +e−(w·x+b) = 1\\n1 +e−(.5∗.2+.6∗.3+.1∗.9+.5) = 1\\n1 +e−0.87 = .70\\nIn practice, the sigmoid is not commonly used as an activation function. A function\\nthat is very similar but almost always better is the tanh function shown in Fig. 6.3a;tanh\\ntanh is a variant of the sigmoid that ranges from -1 to +1:\\ny = tanh(z) =ez −e−z\\nez +e−z (6.5)\\nThe simplest activation function, and perhaps the most commonly used, is the rec-\\ntiﬁed linear unit, also called the ReLU, shown in Fig. 6.3b. It’s just the same as zReLU\\nwhen z is positive, and 0 otherwise:\\ny = ReLU(z) = max(z,0) (6.6)\\nThese activation functions have different properties that make them useful for differ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='when z is positive, and 0 otherwise:\\ny = ReLU(z) = max(z,0) (6.6)\\nThese activation functions have different properties that make them useful for differ-\\nent language applications or network architectures. For example, the tanh function\\nhas the nice properties of being smoothly differentiable and mapping outlier values\\ntoward the mean. The rectiﬁer function, on the other hand, has nice properties that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4 CHAPTER 6 • N EURAL NETWORKS\\n(a) (b)\\nFigure 6.3 The tanh and ReLU activation functions.\\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\\nvalues of z result in values ofy that are saturated, i.e., extremely close to 1, and havesaturated\\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\\nwe’ll see in Section 6.6, we’ll train networks by propagating an error signal back-\\nwards, multiplying gradients (partial derivatives) from each layer of the network;\\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\\ntoo small to be used for training, a problem called the vanishing gradient problem.vanishing\\ngradient\\nRectiﬁers don’t have this problem, since the derivative of ReLU for high values ofz\\nis 1 rather than very close to 0.\\n6.2 The XOR problem\\nEarly in the history of neural networks it was realized that the power of neural net-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='is 1 rather than very close to 0.\\n6.2 The XOR problem\\nEarly in the history of neural networks it was realized that the power of neural net-\\nworks, as with the real neurons that inspired them, comes from combining these\\nunits into larger networks.\\nOne of the most clever demonstrations of the need for multi-layer networks was\\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\\nsome very simple functions of its input. Consider the task of computing elementary\\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\\nthe truth tables for those functions:\\nAND OR XOR\\nx1 x2 y x1 x2 y x1 x2 y\\n0 0 0 0 0 0 0 0 0\\n0 1 0 0 1 1 0 1 1\\n1 0 0 1 0 1 1 0 1\\n1 1 1 1 1 1 1 1 0\\nThis example was ﬁrst shown for the perceptron, which is a very simple neuralperceptron\\nunit that has a binary output and has a very simple step function as its non-linear\\nactivation function. The output y of a perceptron is 0 or 1, and is computed as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='unit that has a binary output and has a very simple step function as its non-linear\\nactivation function. The output y of a perceptron is 0 or 1, and is computed as\\nfollows (using the same weight w, input x, and bias b as in Eq. 6.2):\\ny =\\n{0, if w ·x+b ≤0\\n1, if w ·x+b > 0 (6.7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.2 • T HE XOR PROBLEM 5\\nIt’s very easy to build a perceptron that can compute the logical AND and OR\\nfunctions of its binary inputs; Fig. 6.4 shows the necessary weights.\\nx1\\nx2\\n+1\\n-1\\n1\\n1\\nx1\\nx2\\n+1\\n0\\n1\\n1\\n(a) (b)\\nFigure 6.4 The weights w and bias b for perceptrons for computing logical functions. The\\ninputs are shown asx1 and x2 and the bias as a special node with value+1 which is multiplied\\nwith the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight\\nb = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\\nweights/biases are just one from an inﬁnite number of possible sets of weights and biases that\\nwould implement the functions.\\nIt turns out, however, that it’s not possible to build a perceptron to compute\\nlogical XOR! (It’s worth spending a moment to give it a try!)\\nThe intuition behind this important result relies on understanding that a percep-\\ntron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='The intuition behind this important result relies on understanding that a percep-\\ntron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron\\nequation, w1x1 +w2x2 +b = 0 is the equation of a line. (We can see this by putting\\nit in the standard linear format: x2 = (−w1/w2)x1 + (−b/w2).) This line acts as a\\ndecision boundary in two-dimensional space in which the output 0 is assigned to alldecision\\nboundary\\ninputs lying on one side of the line, and the output 1 to all input points lying on the\\nother side of the line. If we had more than 2 inputs, the decision boundary becomes\\na hyperplane instead of a line, but the idea is the same, separating the space into two\\ncategories.\\nFig. 6.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\\nby one possible set of parameters for an AND and an OR classiﬁer. Notice that there\\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='by one possible set of parameters for an AND and an OR classiﬁer. Notice that there\\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\\nseparable\\nfunction. Of course we could draw a boundary with a curve, or some other function,\\nbut not a single line.\\n6.2.1 The solution: neural networks\\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\\nculated by a layered network of perceptron units. Rather than see this with networks\\nof simple perceptrons, however, let’s see how to compute XOR using two layers of\\nReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a ﬁgure with\\nthe input being processed by two layers of neural units. The middle layer (called\\nh) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='h) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.\\nLet’s walk through what happens with the input x = [0, 0]. If we multiply each\\ninput value by the appropriate weight, sum, and then add the biasb, we get the vector\\n[0, -1], and we then apply the rectiﬁed linear transformation to give the output of the\\nh layer as [0, 0]. Now we once again multiply by the weights, sum, and add the\\nbias (0 in this case) resulting in the value 0. The reader should work through the\\ncomputation of the remaining 3 possible input pairs to see that the resultingy values\\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6 CHAPTER 6 • N EURAL NETWORKS\\n0\\n0 1\\n1\\nx1\\nx2\\n0\\n0 1\\n1\\nx1\\nx2\\n0\\n0 1\\n1\\nx1\\nx2\\na)  x1 AND x2 b)  x1 OR x2 c)  x1 XOR x2\\n?\\nFigure 6.5 The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the\\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\\n(2002).\\nx1\\nx2\\nh1\\nh2\\ny1\\n+1\\n1\\n-1\\n1\\n1\\n1\\n-2\\n0\\n1\\n+1\\n0\\nFigure 6.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\\ntwo layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers\\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\\non a unit clamped to +1, with the bias weights/units in gray.\\nIt’s also instructive to look at the intermediate results, the outputs of the two\\nhidden nodes h1 and h2. We showed in the previous paragraph that the h vector for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='It’s also instructive to look at the intermediate results, the outputs of the two\\nhidden nodes h1 and h2. We showed in the previous paragraph that the h vector for\\nthe inputs x = [0, 0] was [0, 0]. Fig. 6.7b shows the values of the h layer for all\\n4 inputs. Notice that hidden representations of the two input points x = [0, 1] and\\nx = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =\\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\\nof XOR. In other words, we can view the hidden layer of the network as forming a\\nrepresentation of the input.\\nIn this example we just stipulated the weights in Fig. 6.6. But for real examples\\nthe weights for neural networks are learned automatically using the error backprop-\\nagation algorithm to be introduced in Section 6.6. That means the hidden layers will\\nlearn to form useful representations. This intuition, that neural networks can auto-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='agation algorithm to be introduced in Section 6.6. That means the hidden layers will\\nlearn to form useful representations. This intuition, that neural networks can auto-\\nmatically learn useful representations of the input, is one of their key advantages,\\nand one that we will return to again and again in later chapters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.3 • F EEDFORWARD NEURAL NETWORKS 7\\n0\\n0 1\\n1\\nx1\\nx2\\na) The original x space\\n0\\n0 1\\n1\\nh1\\nh2\\n2\\nb) The new (linearly separable) h space\\nFigure 6.7 The hidden layer forming a new representation of the input. (b) shows the\\nrepresentation of the hidden layer, h, compared to the original input representation x in (a).\\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\\n(2016).\\n6.3 Feedforward Neural Networks\\nLet’s now walk through a slightly more formal presentation of the simplest kind of\\nneural network, the feedforward network. A feedforward network is a multilayerfeedforward\\nnetwork\\nnetwork in which the units are connected with no cycles; the outputs from units in\\neach layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='each layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)\\nFor historical reasons multilayer networks, especially feedforward networks, are\\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,multi-layer\\nperceptrons\\nMLP since the units in modern multilayer networks aren’t perceptrons (perceptrons have a\\nsimple step-function as their activation function, but modern networks are made up\\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\\npoint the name stuck.\\nSimple feedforward networks have three kinds of nodes: input units, hidden\\nunits, and output units.\\nFig. 6.8 shows a picture. The input layerx is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is thehidden layer h formed of hidden units hi,hidden layer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Fig. 6.8 shows a picture. The input layerx is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is thehidden layer h formed of hidden units hi,hidden layer\\neach of which is a neural unit as described in Section 6.1, taking a weighted sum of\\nits inputs and then applying a non-linearity. In the standard architecture, each layer\\nis fully-connected, meaning that each unit in each layer takes as input the outputsfully-connected\\nfrom all the units in the previous layer, and there is a link between every pair of units\\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\\nrepresent the parameters for the entire hidden layer by combining the weight vector\\nand bias for each unit i into a single weight matrix W and a single bias vector b for\\nthe whole layer (see Fig. 6.8). Each element Wji of the weight matrix W represents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='and bias for each unit i into a single weight matrix W and a single bias vector b for\\nthe whole layer (see Fig. 6.8). Each element Wji of the weight matrix W represents\\nthe weight of the connection from the ith input unit xi to the jth hidden unit hj.\\nThe advantage of using a single matrix W for the weights of the entire layer is\\nthat now the hidden layer computation for a feedforward network can be done very'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='8 CHAPTER 6 • N EURAL NETWORKS\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW\\ninput layer hidden layer output layer\\nh1\\ny1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nFigure 6.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\\nand one input layer (the input layer is usually not counted when enumerating layers).\\nefﬁciently with simple matrix operations. In fact, the computation only has three\\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\\nand applying the activation functiong (such as the sigmoid, tanh, or ReLU activation\\nfunction deﬁned above).\\nThe output of the hidden layer, the vectorh, is thus the following (for this exam-\\nple we’ll use the sigmoid functionσ as our activation function):\\nh = σ(Wx+b) (6.8)\\nNotice that we’re applying the σ function here to a vector, while in Eq. 6.3 it was\\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function\\ng(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='applied to a scalar. We’re thus allowing σ(·), and indeed any activation function\\ng(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].\\nLet’s introduce some constants to represent the dimensionalities of these vectors\\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have\\nn0 represent the number of inputs, so x is a vector of real numbers of dimension\\nn0, or more formally x ∈Rn0 , a column vector of dimensionality [n0 ×1]. Let’s\\ncall the hidden layer layer 1 and the output layer layer 2. The hidden layer has\\ndimensionality n1, so h ∈Rn1 and also b ∈Rn1 (since each hidden unit can take a\\ndifferent bias value). And the weight matrix W has dimensionality W ∈Rn1×n0 , i.e.\\n[n1 ×n0].\\nTake a moment to convince yourself that the matrix multiplication in Eq. 6.8 will\\ncompute the value of each hj as σ\\n(∑n0\\ni=1 Wjixi +bj\\n)\\n.\\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypoth-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='compute the value of each hj as σ\\n(∑n0\\ni=1 Wjixi +bj\\n)\\n.\\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypoth-\\nesis) forms a representation of the input. The role of the output layer is to take\\nthis new representation h and compute a ﬁnal output. This output could be a real-\\nvalued number, but in many cases the goal of the network is to make some sort of\\nclassiﬁcation decision, and so we will focus on the case of classiﬁcation.\\nIf we are doing a binary task like sentiment classiﬁcation, we might have a sin-\\ngle output node, and its scalar value y is the probability of positive versus negative\\nsentiment. If we are doing multinomial classiﬁcation, such as assigning a part-of-\\nspeech tag, we might have one output node for each potential part-of-speech, whose\\noutput value is the probability of that part-of-speech, and the values of all the output\\nnodes must sum to one. The output layer is thus a vector y that gives a probability'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='output value is the probability of that part-of-speech, and the values of all the output\\nnodes must sum to one. The output layer is thus a vector y that gives a probability\\ndistribution across the output nodes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.3 • F EEDFORWARD NEURAL NETWORKS 9\\nLet’s see how this happens. Like the hidden layer, the output layer has a weight\\nmatrix (let’s call it U), but some models don’t include a bias vector b in the output\\nlayer, so we’ll simplify by eliminating the bias vector in this example. The weight\\nmatrix is multiplied by its input vector (h) to produce the intermediate output z:\\nz = Uh\\nThere are n2 output nodes, so z ∈Rn2 , weight matrix U has dimensionality U ∈\\nRn2×n1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the\\noutput layer.\\nHowever, z can’t be the output of the classiﬁer, since it’s a vector of real-valued\\nnumbers, while what we need for classiﬁcation is a vector of probabilities. There is\\na convenient function for normalizing a vector of real values, by which we meannormalizing\\nconverting it to a vector that encodes a probability distribution (all the numbers lie\\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? ofsoftmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='converting it to a vector that encodes a probability distribution (all the numbers lie\\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? ofsoftmax\\nChapter 4. More generally for any vector z of dimensionality d, the softmax is\\ndeﬁned as:\\nsoftmax(zi) = exp(zi)∑d\\nj=1 exp(zj)\\n1 ≤i ≤d (6.9)\\nThus for example given a vector\\nz = [0.6,1.1,−1.5,1.2,3.2,−1.1], (6.10)\\nthe softmax function will normalize it to a probability distribution (shown rounded):\\nsoftmax(z) = [0.055,0.090,0.0067,0.10,0.74,0.010] (6.11)\\nYou may recall that we used softmax to create a probability distribution from a\\nvector of real-valued numbers (computed from summing weights times features) in\\nthe multinomial version of logistic regression in Chapter 4.\\nThat means we can think of a neural network classiﬁer with one hidden layer\\nas building a vector h which is a hidden layer representation of the input, and then\\nrunning standard multinomial logistic regression on the features that the network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='as building a vector h which is a hidden layer representation of the input, and then\\nrunning standard multinomial logistic regression on the features that the network\\ndevelops in h. By contrast, in Chapter 4 the features were mainly designed by hand\\nvia feature templates. So a neural network is like multinomial logistic regression,\\nbut (a) with many layers, since a deep neural network is like layer after layer of lo-\\ngistic regression classiﬁers; (b) with those intermediate layers having many possible\\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll\\ncontinue to use σ for convenience to mean any activation function); (c) rather than\\nforming the features by feature templates, the prior layers of the network induce the\\nfeature representations themselves.\\nHere are the ﬁnal equations for a feedforward network with a single hidden layer,\\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Here are the ﬁnal equations for a feedforward network with a single hidden layer,\\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\\nized by weight matrices W and U and a bias vector b:\\nh = σ(Wx+b)\\nz = Uh\\ny = softmax(z) (6.12)\\nAnd just to remember the shapes of all our variables, x ∈Rn0 , h ∈Rn1 , b ∈Rn1 ,\\nW ∈Rn1×n0 , U ∈Rn2×n1 , and the output vectory ∈Rn2 . We’ll call this network a 2-\\nlayer network (we traditionally don’t count the input layer when numbering layers,\\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\\nnetwork.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='10 CHAPTER 6 • N EURAL NETWORKS\\n6.3.1 More details on feedforward networks\\nLet’s now set up some notation to make it easier to talk about deeper networks of\\ndepth more than 2. We’ll use superscripts in square brackets to mean layer num-\\nbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\\n(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. nj\\nwill mean the number of units at layer j. We’ll use g(·) to stand for the activation\\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\\nfor output layers. We’ll usea[i] to mean the output from layer i, and z[i] to mean the\\ncombination of previous layer output, weights and biases W[i]a[i−1] + b[i]. The 0th\\nlayer is for inputs, so we’ll refer to the inputsx more generally as a[0].\\nThus we can re-represent our 2-layer net from Eq. 6.12 as follows:\\nz[1] = W[1]a[0] +b[1]\\na[1] = g[1](z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = g[2](z[2])\\nˆy = a[2] (6.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Thus we can re-represent our 2-layer net from Eq. 6.12 as follows:\\nz[1] = W[1]a[0] +b[1]\\na[1] = g[1](z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = g[2](z[2])\\nˆy = a[2] (6.13)\\nNote that with this notation, the equations for the computation done at each layer are\\nthe same. The algorithm for computing the forward step in an n-layer feedforward\\nnetwork, given the input vector a[0] is thus simply:\\nfor i in 1,...,n\\nz[i] = W[i] a[i−1] + b[i]\\na[i] = g[i](z[i])\\nˆy = a[n]\\nIt’s often useful to have a name for the ﬁnal set of activations right before the ﬁnal\\nsoftmax. So however many layers we have, we’ll generally call the unnormalized\\nvalues in the ﬁnal vector z[n], the vector of scores right before the ﬁnal softmax, the\\nlogits (see Eq. ??).logits\\nThe need for non-linear activation functions One of the reasons we use non-\\nlinear activation functions for each layer in a neural network is that if we did not, the\\nresulting network is exactly equivalent to a single-layer network. Let’s see why this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='linear activation functions for each layer in a neural network is that if we did not, the\\nresulting network is exactly equivalent to a single-layer network. Let’s see why this\\nis true. Imagine the ﬁrst two layers of such a network of purely linear layers:\\nz[1] = W[1]x+b[1]\\nz[2] = W[2]z[1] +b[2]\\nWe can rewrite the function that the network is computing as:\\nz[2] = W[2]z[1] +b[2]\\n= W[2](W[1]x+b[1])+ b[2]\\n= W[2]W[1]x+W[2]b[1] +b[2]\\n= W′x+b′ (6.14)\\nThis generalizes to any number of layers. So without non-linear activation functions,\\na multilayer network is just a notational variant of a single layer network with a\\ndifferent set of weights, and we lose all the representational power of multilayer\\nnetworks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.4 • F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 11\\nReplacing the bias unit In describing networks, we will sometimes use a slightly\\nsimpliﬁed notation that represents exactly the same function without referring to an\\nexplicit bias node b. Instead, we add a dummy node a0 to each layer whose value\\nwill always be 1. Thus layer 0, the input layer, will have a dummy node a[0]\\n0 = 1,\\nlayer 1 will havea[1]\\n0 = 1, and so on. This dummy node still has an associated weight,\\nand that weight represents the bias value b. For example instead of an equation like\\nh = σ(Wx+b) (6.15)\\nwe’ll use:\\nh = σ(Wx) (6.16)\\nBut now instead of our vector x having n0 values: x = x1,..., xn0 , it will have n0 +\\n1 values, with a new 0th dummy value x0 = 1: x = x0,..., xn0 . And instead of\\ncomputing each hj as follows:\\nhj = σ\\n(n0∑\\ni=1\\nWji xi +bj\\n)\\n, (6.17)\\nwe’ll instead use:\\nhj = σ\\n(n0∑\\ni=0\\nWji xi\\n)\\n, (6.18)\\nwhere the value Wj0 replaces what had been bj. Fig. 6.9 shows a visualization.\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW h1 y1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='hj = σ\\n(n0∑\\ni=1\\nWji xi +bj\\n)\\n, (6.17)\\nwe’ll instead use:\\nhj = σ\\n(n0∑\\ni=0\\nWji xi\\n)\\n, (6.18)\\nwhere the value Wj0 replaces what had been bj. Fig. 6.9 shows a visualization.\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW h1 y1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nx1\\nx2\\nxn0\\n…\\n…\\nx0=1\\n…\\nUW\\nh1 y1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\n(a) (b)\\nFigure 6.9 Replacing the bias node (shown in a) with x0 (b).\\nWe’ll continue showing the bias as b when we go over the learning algorithm\\nin Section 6.6, but going forward in the book, for most ﬁgures and some equations\\nwe’ll use this simpliﬁed notation without explicit bias terms.\\n6.4 Feedforward networks for NLP: Classiﬁcation\\nLet’s see how to apply feedforward networks to NLP classiﬁcation tasks. In practice,\\nsimple feedforward networks aren’t the way we do text classiﬁcation; for real appli-\\ncations we would use more sophisticated architectures like the BERT transformers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='12 CHAPTER 6 • N EURAL NETWORKS\\nof Chapter 10. Nonetheless seeing a feedforward network text classiﬁer will let us\\nintroduce key ideas that will play a role throughout the rest of the book, includ-\\ning the ideas of theembedding matrix, representation pooling, and representation\\nlearning.\\nBut before introducing any of these ideas, let’s start with a classiﬁer by making\\nonly minimal change from the sentiment classiﬁers we saw in Chapter 4. Like them,\\nwe’ll take hand-built features, pass them through a classiﬁer, and produce a class\\nprobability. The only difference is that we’ll use a neural network instead of logistic\\nregression as the classiﬁer.\\n6.4.1 Neural net classiﬁers with hand-built features\\nLet’s begin with a simple 2-layer sentiment classiﬁer by taking our logistic regres-\\nsion classiﬁer from Chapter 4, which corresponds to a 1-layer network, and just\\nadding a hidden layer. The input element xi can be scalar features like those in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sion classiﬁer from Chapter 4, which corresponds to a 1-layer network, and just\\nadding a hidden layer. The input element xi can be scalar features like those in\\nFig. ??, e.g., x1 = count(words ∈doc), x2 = count(positive lexicon words ∈doc),\\nx3 = 1 if “no” ∈doc, and so on, for a total of d features. And the output layer\\nˆy could have two nodes (one each for positive and negative), or 3 nodes (positive,\\nnegative, neutral), in which case ˆy1 would be the estimated probability of positive\\nsentiment, ˆy2 the probability of negative and ˆy3 the probability of neutral. The re-\\nsulting equations would be just what we saw above for a 2-layer network (as always,\\nwe’ll continue to use the σ to stand for any non-linearity, whether sigmoid, ReLU\\nor other).\\nx = [x1,x2,...xd] (each xi is a hand-designed feature)\\nh = σ(Wx+b)\\nz = Uh\\nˆy = softmax(z) (6.19)\\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='or other).\\nx = [x1,x2,...xd] (each xi is a hand-designed feature)\\nh = σ(Wx+b)\\nz = Uh\\nˆy = softmax(z) (6.19)\\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this\\nhidden layer to our logistic regression classiﬁer allows the network to represent the\\nnon-linear interactions between features. This alone might give us a better sentiment\\nclassiﬁer.\\nUW\\n[d⨉1]\\nHidden layer Output layer\\nsoftmax\\n[dh⨉d] [dh⨉1] [3⨉dh]\\nInput words\\np(+)\\nh1\\nh2\\nh3\\nhdh …\\ny1^\\ny2^\\ny3^\\nx h y\\nInput layer \\nd=3 features\\n[3⨉1]\\nx1\\nx2\\nx3\\ndessert\\nwas\\ngreat\\npositive lexicon\\nwords = 1\\ncount of “no” \\n= 0\\nwordcount\\n=3\\np(-)\\np(neut)\\nFigure 6.10 Feedforward network sentiment analysis using traditional hand-built features\\nof the input text.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 13\\n6.4.2 Vectorizing for parallelizing inference\\nWhile Eq. 6.19 shows how to classify a single example x, in practice we want to\\nefﬁciently classify an entire test set of m examples. We do this by vectorizing the\\nprocess, just as we saw with logistic regression; instead of using for-loops to go\\nthrough each example, we’ll use matrix multiplication to do the entire computation\\nof an entire test set at once. First, we pack all the input feature vectors for each input\\nx into a single input matrixX, with each rowi a row vector consisting of the features\\nfor input example x(i) (i.e., the vector x(i)). If the dimensionality of our input feature\\nvector is d, X will be a matrix of shape [m ×d].\\nBecause we are now modeling each input as a row vector rather than a column\\nvector, we also need to slightly modify Eq. 6.19. X is of shape [m ×d] and W is of\\nshape [dh ×d], so we’ll reorder how we multiplyX and W and transpose W so they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='vector, we also need to slightly modify Eq. 6.19. X is of shape [m ×d] and W is of\\nshape [dh ×d], so we’ll reorder how we multiplyX and W and transpose W so they\\ncorrectly multiply to yield a matrix H of shape [m ×dh]. 1\\nThe bias vector b from Eq. 6.19 of shape [1 ×dh] will now have to be replicated\\ninto a matrix of shape [m ×dh]. We’ll need to similarly reorder the next step and\\ntranspose U. Finally, our output matrix ˆY will be of shape [m ×3] (or more gen-\\nerally [m ×do], where do is the number of output classes), with each row i of our\\noutput matrix ˆY consisting of the output vector ˆy(i). Here are the ﬁnal equations for\\ncomputing the output class distribution for an entire test set:\\nH = σ(XW⊺ +b)\\nZ = HU⊺\\nˆY = softmax(Z) (6.20)\\nIn this book, we’ll sometimes see orderings like WX + b and sometimes XW + b.\\nThat’s why it’s always important to be very aware of the shapes of your weight\\nmatrices participating in any given equation.\\n6.5 Embeddings as the input to neural net classiﬁers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='That’s why it’s always important to be very aware of the shapes of your weight\\nmatrices participating in any given equation.\\n6.5 Embeddings as the input to neural net classiﬁers\\nWhile hand-built features are a traditional way to design classiﬁers, most applica-\\ntions of neural networks for NLP don’t use hand-built human-engineered features as\\ninputs. Instead, we draw on deep learning’s ability to learn features from the data by\\nrepresenting tokens as embeddings. For this section we’ll represent each token by\\nits static word2vec or GloVe embeddings that we saw how to compute in Chapter 5.\\nBy static embedding, we mean that each token is represented by a ﬁxed vector that\\nwe train once, and then just put into a big dictionary. When we want to refer to that\\ntoken, we grab its embedding out of the dictionary.\\nHowever when we apply neural models to the task of language modeling (as\\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more power-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='However when we apply neural models to the task of language modeling (as\\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more power-\\nful kind of embedding called a contextual embedding. Contextual embeddings are\\ndifferent for each time a word occurs in a different context. Furthermore, we’ll have\\nthe network learn these embeddings as part of the task of word prediction.\\nSo let’s explore the text classiﬁcation domain above, but using static embeddings\\nas features instead of the hand-designed features. Let’s focus on the inference stage,\\n1 Note that we could have kept the original order of our products if we had instead made our input\\nmatrix X represent each input as a column vector instead of a row vector, making it of shape[d ×m]. But\\nrepresenting inputs as row vectors is convenient and common in neural network models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='14 CHAPTER 6 • N EURAL NETWORKS\\nin which we have already learned embeddings for all the input tokens. An embed-\\nding is a vector of dimension d that represents the input token. The dictionary of\\nstatic embeddings in which we store these embeddings is the embedding matrixembedding\\nmatrix\\nE. Each row of the embedding matrix represents each token of the vocabulary V\\nas a (row) vector of dimensionality d. Since E has a row for each of the |V |to-\\nkens in the vocabulary, E has shape [|V |×d]. This embedding matrix E plays a role\\nwhenever we are using embeddings as input to neural NLP systems, including in the\\ntransformer-based large language models we will introduce over the next chapters.\\nGiven an input token string likedessert was great we ﬁrst convert the tokens\\ninto vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of dessert was great might be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='into vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of dessert was great might be\\nw = [3,9824,226]. Next we use indexing to select the corresponding rows from E\\n(row 3, row 4000, row 10532).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent input tokens as one-hot vectors of shape [1 ×|V |], i.e., with\\none dimension for each word in the vocabulary. Recall that in a one-hot vector allone-hot vector\\nthe elements are 0 except one, the element whose dimension is the word’s index\\nin the vocabulary, which has value 1. So if the word “dessert” has index 3 in the\\nvocabulary, x3 = 1, and xi = 0 ∀i ̸= 3, as shown here:\\n[0 0 1 0 0 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ... ... |V|\\nMultiplying by a one-hot vector that has only one non-zero elementxi = 1 simply\\nselects out the relevant row vector for wordi, resulting in the embedding for word i,\\nas depicted in Fig. 6.11.\\nE\\n|V|\\nd\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='selects out the relevant row vector for wordi, resulting in the embedding for word i,\\nas depicted in Fig. 6.11.\\nE\\n|V|\\nd\\n1\\n|V| d\\n=✕\\n33\\n0 0 1 0 0 0 0 … 0 0 0 0 1\\nFigure 6.11 Selecting the embedding vector for word V3 by multiplying the embedding\\nmatrix E with a one-hot vector with a 1 in index 3.\\nWe can extend this idea to represent the entire input token sequence as a matrix\\nof one-hot vectors, one for each of the N input positions as shown in Fig. 6.12.\\nE\\n|V|\\nd\\nd\\nN\\n=✕\\n|V|\\nN\\n0 0 0 0 0 0 0 … 0 0 1 0 \\n0 0 1 0 0 0 0 … 0 0 0 0 \\n1 0 0 0 0 0 0 … 0 0 0 0 \\n0 0 0 0 1 0 0 … 0 0 0 0 \\n…\\nFigure 6.12 Selecting the embedding matrix for the input sequence of token idsW by mul-\\ntiplying a one-hot matrix corresponding to W by the embedding matrix E.\\nWe now need to classify this input ofN [1 ×d] embeddings, representing a win-\\ndow of N tokens, into a single class (like positive or negative).\\nThere are two common ways to to pass embeddings to a classiﬁer: concate-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dow of N tokens, into a single class (like positive or negative).\\nThere are two common ways to to pass embeddings to a classiﬁer: concate-\\nnation and pooling. First, we can take this input of shape [N ×d] and reshape it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 15\\nby concatenating all the input vectors into one very long vector of shape [1 ×dN].\\nThen we pass this input to our classiﬁer and let it make its decision. This gives\\nus lots of information, at the cost of using a pretty large network. Second, we can\\npool the N embeddings into a single embedding and then pass that single pooledpool\\nembedding to the classiﬁer. Pooling gives us less information than would have been\\npresent in all the original embeddings, but has the advantage of being small and ef-\\nﬁcient and is especially useful in tasks for which we don’t care as much about the\\noriginal word order. Let’s give an example of each: pooling for the sentiment task,\\nand concatenation for the language modeling task.\\nPooling input embeddings for sentiment So let’s begin with seeing how pooling\\ncan work for the sentiment classiﬁcation task. The intuition of pooling is that for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Pooling input embeddings for sentiment So let’s begin with seeing how pooling\\ncan work for the sentiment classiﬁcation task. The intuition of pooling is that for\\nsentiment, the exact position of the input (is some word like great the ﬁrst word?\\nthe second word?) is less important than the identity of the word itself.\\nA pooling function is a way to turn a set of embeddings into a single embedding.\\nFor example, for a text with N input words/tokens w1,..., wN , we want to turn\\nthe N row embeddings e(w1),..., e(wN ) (each of dimensionality d) into a single\\nembedding also of dimensionality d.\\nThere are various ways to pool. The simplest is mean-pooling: taking the meanmean-pooling\\nby summing the embeddings and then dividing by N:\\nxmean = 1\\nN\\nN∑\\ni=1\\ne(wi) (6.21)\\nHere are the equations for this classiﬁer assuming mean pooling:\\nx = mean(e(w1),e(w2),..., e(wn))\\nh = σ(xW +b)\\nz = hU\\nˆy = softmax(z) (6.22)\\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='x = mean(e(w1),e(w2),..., e(wn))\\nh = σ(xW +b)\\nz = hU\\nˆy = softmax(z) (6.22)\\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the\\nrelevant matrices.\\nThere are many other options for pooling, like max-pooling, in which case formax-pooling\\neach dimension we take the element-wise max over all the inputs. The element-wise\\nmax of a set of N vectors is a new vector whose kth element is the max of the kth\\nelements of all the N vectors.\\nConcatenating input embeddings for language modeling For sentiment analy-\\nsis we saw how to generate an output vector with probabilities over three classes:\\npositive, negative, or neutral, given as input a window of N input tokens, by ﬁrst\\npooling those token embeddings into a single embedding vector.\\nNow let’s considerlanguage modeling: predicting upcoming words from prior\\nwords. In this task we are given the same window of N input tokens, but our task'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Now let’s considerlanguage modeling: predicting upcoming words from prior\\nwords. In this task we are given the same window of N input tokens, but our task\\nnow is to predict the next token that should follow the window. We’ll sketch a\\nsimple feedforward neural language model, drawing on an algorithm ﬁrst introduced\\nby Bengio et al. (2003). The feedforward language model introduces many of the\\nimportant concepts of large language modeling that we will return to in Chapter 7\\nand Chapter 8.\\nNeural language models have many advantages over the n-gram language mod-\\nels of Chapter 3. Neural language models can handle much longer histories, can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='16 CHAPTER 6 • N EURAL NETWORKS\\n“dessert” = V3 “was” = V524 “great” = V902\\nembedding for “dessert”\\nembedding for “was”\\nembedding for “great”\\nU\\nW\\n[1⨉d]\\nHidden layer\\nOutput layer\\n[d⨉dh]\\n[1⨉dh]\\n[dh⨉3]\\nInput words\\np(+)\\nh1 h2 h3 hdh\\n…\\ny1\\n^ y2^ y3^\\nx\\nh\\ny\\nInput layer \\n[1⨉3]\\npooling+\\np(-) p(neut)\\nembeddings\\none-hot vectors\\ndessert was great\\nN⨉d\\n0 0 1 00\\n1 |V|3\\n0 0 1 00\\n1 |V|902\\n0 0 1 00\\n1 |V|524\\n0\\n0\\nE\\nN⨉|V|\\n|V|⨉dE E E matrix\\nshared across words\\nOutput probabilities\\nweights\\nweights\\nsoftmax\\npooled embedding\\nFigure 6.13 Feedforward network sentiment analysis using a pooled embedding of the input words. At each\\ntimestep the network computes a d-dimensional embedding for each context word (by multiplying a one-hot\\nvector by the embedding matrix E), and pools the resulting N embeddings to get a single embedding that\\nrepresents the context window as the layer e.\\ngeneralize better over contexts of similar words, and are far more accurate at word-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='represents the context window as the layer e.\\ngeneralize better over contexts of similar words, and are far more accurate at word-\\nprediction. On the other hand, neural net language models are slower, more com-\\nplex, need vast amounts of energy to train, and are less interpretable than n-gram\\nmodels, so for some smaller tasks an n-gram language model is still the right tool.\\nA feedforward neural language model is a feedforward network that takes as\\ninput at time t a representation of some number of previous words (wt−1,wt−2, etc.)\\nand outputs a probability distribution over possible next words. Thus—like the n-\\ngram LM—the feedforward neural LM approximates the probability of a word given\\nthe entire prior context P(wt |w1:t−1) by approximating based on the N −1 previous\\nwords:\\nP(wt |w1,..., wt−1) ≈P(wt |wt−N+1,..., wt−1) (6.23)\\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to\\nestimate the probability P(wt = i|wt−3,wt−2,wt−1).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='words:\\nP(wt |w1,..., wt−1) ≈P(wt |wt−N+1,..., wt−1) (6.23)\\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to\\nestimate the probability P(wt = i|wt−3,wt−2,wt−1).\\nNeural language models represent words in this prior context by their embed-\\ndings, rather than just by their word identity as used in n-gram language models.\\nUsing embeddings allows neural language models to generalize better to unseen\\ndata. For example, suppose we’ve seen this sentence in training:\\nI have to make sure that the cat gets fed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 17\\nbut have never seen the words “gets fed” after the word “dog”. Our test set has the\\npreﬁx “I forgot to make sure that the dog gets”. What’s the next word? An n-gram\\nlanguage model will predict “fed” after “that the cat gets”, but not after “that the dog\\ngets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will\\nbe able to generalize from the “cat” context to assign a high enough probability to\\n“fed” even after seeing “dog”.\\nh1 h2\\ny1\\nh3 hdh…\\n…\\nU\\nW\\ny34 y|V|\\nembedding layer e 1⨉Nd\\nhidden layer h\\noutput layer y\\nsoftmax\\n…\\n...\\nwt-1wt-2 wtwt-3\\nNd⨉dh\\n1⨉dh\\ndh⨉|V|\\n1⨉|V|\\nInput layer\\none-hot \\nvectors “for” = V35\\n0 0 1 00\\n1 |V|35\\n0 0 1 00\\n1 |V|451\\n0 0 1 00\\n1 |V|992\\n0 0\\n“all” = V992 “the” = V451\\nE\\nN⨉|V|\\nE is shared\\nacross words |V|⨉d\\n…\\np(wt=do|…)p(wt=aardvark|wt-3,wt-2,wt-1) p(wt=zebra|…)p(wt=fish|…)\\n… y42 y35102^^^ ^ ^\\nE E\\nfor all the ?thanksand… …'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='1 |V|992\\n0 0\\n“all” = V992 “the” = V451\\nE\\nN⨉|V|\\nE is shared\\nacross words |V|⨉d\\n…\\np(wt=do|…)p(wt=aardvark|wt-3,wt-2,wt-1) p(wt=zebra|…)p(wt=fish|…)\\n… y42 y35102^^^ ^ ^\\nE E\\nfor all the ?thanksand… …\\nFigure 6.14 Forward inference in a feedforward neural language model. At each timestep\\nt the network computes a d-dimensional embedding for each of the N = 3 context tokens (by\\nmultiplying a one-hot vector by the embedding matrix E), and concatenates the three to get\\nthe embedding e. This embedding e is multiplied by weight matrix W and then an activation\\nfunction is applied element-wise to produce the hidden layer h, which is then multiplied by\\nanother weight matrix U. A softmax layer predicts at each output node i the probability that\\nthe next word wt will be vocabulary wordVi. We show the context window sizeN as 3 just to\\nﬁt on the page, but in practice language modeling requires a much longer context.\\nThis prediction task requires an output vector that expresses |V |probabilities:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ﬁt on the page, but in practice language modeling requires a much longer context.\\nThis prediction task requires an output vector that expresses |V |probabilities:\\none probability value for each possible next token. We might have a vocabulary\\nbetween 60,000 and 300,000 tokens, so the output vector for the task of language\\nmodeling is much longer than 3. Another difference for language modeling is that\\ninstead of pooling the embeddings of the N input tokens to create a single embed-\\nding, we concatenate the inputs into one very long input vector. To predict the next\\ntoken, it helps to know each of the preceding tokens and what order they were in.\\nFig. 6.14 shows the language modeling task, sketched with a very short context\\nwindow of N = 3 just to ﬁt on the page. These 3 embedding vectors are concatenated\\nto produce e, the embedding layer. This is multiplied by a weight matrix W to pro-\\nduce a hidden layer, and another weight matrix U to produce an output layer whose'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='to produce e, the embedding layer. This is multiplied by a weight matrix W to pro-\\nduce a hidden layer, and another weight matrix U to produce an output layer whose\\nsoftmax gives a probability distribution over words. For example y42, the value of\\noutput node 42, is the probability of the next wordwt being V42, the vocabulary word\\nwith index 42 (which is the word ‘ﬁsh’ in our example).\\nThe equations for a simple feedforward neural language model with a window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='18 CHAPTER 6 • N EURAL NETWORKS\\nsize of 3, given one-hot input vectors for each input context word, are:\\ne = [Ext−3;Ext−2;Ext−1]\\nh = σ(We+b)\\nz = Uh\\nˆy = softmax(z) (6.24)\\nNote that we we use semicolons to mean concatenation of vectors, so we form the\\nembedding layer e by concatenating the 3 embeddings for the three context vectors.\\nWe’ll return to this idea of using neural networks to do language modeling in\\nChapter 7 and Chapter 8 when we introduce transformer language models.\\n6.6 Training Neural Nets\\nA feedforward neural net is an instance of supervised machine learning in which we\\nknow the correct output y for each observation x. What the system produces, via\\nEq. 6.13, is ˆy, the system’s estimate of the truey. The goal of the training procedure\\nis to learn parameters W[i] and b[i] for each layer i that make ˆy for each training\\nobservation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='observation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4\\nfor logistic regression, so the reader should be comfortable with that chapter before\\nproceeding. We’ll explore the algorithm on simple generic networks rather than\\nnetworks designed for sentiment or language modeling.\\nFirst, we’ll need a loss function that models the distance between the system\\noutput and the gold output, and it’s common to use the loss function used for logistic\\nregression, the cross-entropy loss.\\nSecond, to ﬁnd the parameters that minimize this loss function, we’ll use the\\ngradient descent optimization algorithm introduced in Chapter 4.\\nThird, gradient descent requires knowing the gradient of the loss function, the\\nvector that contains the partial derivative of the loss function with respect to each\\nof the parameters. In logistic regression, for each observation we could directly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='vector that contains the partial derivative of the loss function with respect to each\\nof the parameters. In logistic regression, for each observation we could directly\\ncompute the derivative of the loss function with respect to an individual w or b. But\\nfor neural networks, with millions of parameters in many layers, it’s much harder to\\nsee how to compute the partial derivative of some weight in layer 1 when the loss\\nis attached to some much later layer. How do we partial out the loss over all those\\nintermediate layers? The answer is the algorithm called error backpropagation or\\nbackward differentiation.\\n6.6.1 Loss function\\nThe cross-entropy loss that is used in neural networks is the same one we saw forcross-entropy\\nloss\\nlogistic regression. If the neural network is being used as a binary classiﬁer, with\\nthe sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss\\nwe saw in Eq. ??:\\nLCE (ˆy,y) =−log p(y|x) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.25)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss\\nwe saw in Eq. ??:\\nLCE (ˆy,y) =−log p(y|x) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.25)\\nIf we are using the network to classify into 3 or more classes, the loss function is\\nexactly the same as the loss for multinomial regression that we saw in Chapter 4 on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 19\\npage ??. Let’s brieﬂy summarize the explanation here for convenience. First, when\\nwe have more than 2 classes we’ll need to represent both y and ˆy as vectors. Let’s\\nassume we’re doing hard classiﬁcation , where only one class is the correct one.\\nThe true label y is then a vector with K elements, each corresponding to a class,\\nwith yc = 1 if the correct class is c, with all other elements of y being 0. Recall that\\na vector like this, with one value equal to 1 and the rest 0, is called aone-hot vector.\\nAnd our classiﬁer will produce an estimate vector with K elements ˆy, each element\\nˆyk of which represents the estimated probability p(yk = 1|x).\\nThe loss function for a single example x is the negative sum of the logs of the K\\noutput classes, each weighted by their probability yk:\\nLCE (ˆy,y) =−\\nK∑\\nk=1\\nyk log ˆyk (6.26)\\nWe can simplify this equation further; let’s ﬁrst rewrite the equation using the func-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='output classes, each weighted by their probability yk:\\nLCE (ˆy,y) =−\\nK∑\\nk=1\\nyk log ˆyk (6.26)\\nWe can simplify this equation further; let’s ﬁrst rewrite the equation using the func-\\ntion 1 {}which evaluates to 1 if the condition in the brackets is true and to 0 oth-\\nerwise. This makes it more obvious that the terms in the sum in Eq. 6.26 will be 0\\nexcept for the term corresponding to the true class for which yk = 1:\\nLCE (ˆy,y) = −\\nK∑\\nk=1\\n1 {yk = 1}log ˆyk\\nIn other words, the cross-entropy loss is simply the negative log of the output proba-\\nbility corresponding to the correct class, and we therefore also call this the negative\\nlog likelihood loss:negative log\\nlikelihood loss\\nLCE (ˆy,y) = −log ˆyc (where c is the correct class) (6.27)\\nPlugging in the softmax formula from Eq. 6.9, and with K the number of classes:\\nLCE (ˆy,y) = −log exp(zc)∑K\\nj=1 exp(zj)\\n(where c is the correct class) (6.28)\\nLet’s think about the negative log probability as a loss function. A perfect clas-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='LCE (ˆy,y) = −log exp(zc)∑K\\nj=1 exp(zj)\\n(where c is the correct class) (6.28)\\nLet’s think about the negative log probability as a loss function. A perfect clas-\\nsiﬁer would assign the correct class i probability 1 and all the incorrect classes prob-\\nability 0. That means the higher p(ˆyi) (the closer it is to 1), the better the classiﬁer;\\np(ˆyi) is (the closer it is to 0), the worse the classiﬁer. The negative log of this prob-\\nability is a beautiful loss metric since it goes from 0 (negative log of 1, no loss)\\nto inﬁnity (negative log of 0, inﬁnite loss). This loss function also insures that as\\nprobability of the correct answer is maximized, the probability of all the incorrect\\nanswers is minimized; since they all sum to one, any increase in the probability of\\nthe correct answer is coming at the expense of the incorrect answers.\\nThe number K of classes of the output vector ˆy can be small or large. Perhaps'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the correct answer is coming at the expense of the incorrect answers.\\nThe number K of classes of the output vector ˆy can be small or large. Perhaps\\nour task is 3-way sentiment, and then the classes might be positive, negative, and\\nneutral. Or if our task is deciding the part of speech of a word (i.e., whether it is a\\nnoun or verb or adjective, etc.), thenK is set of possible parts of speech in our tagset\\n(of which there are 17 in the tagset we will deﬁne in Chapter 17). And if our task\\nis language modeling, and our classiﬁer is trying to predict which word is next, then\\nour set of classes is the set of words, which might be 50,000 or 100,000.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='20 CHAPTER 6 • N EURAL NETWORKS\\n6.6.2 Computing the Gradient\\nHow do we compute the gradient of this loss function? Computing the gradient\\nrequires the partial derivative of the loss function with respect to each parameter.\\nFor a network with one weight layer and sigmoid output (which is what logistic\\nregression is), we could simply use the derivative of the loss that we used for logistic\\nregression in Eq. 6.29 (and derived in Section ??):\\n∂LCE (ˆy,y)\\n∂wj\\n= ( ˆy −y)xj\\n= (σ(w ·x+b)−y)xj (6.29)\\nOr for a network with one weight layer and softmax output (=multinomial logistic\\nregression), we could use the derivative of the softmax loss from Eq. ??, shown for\\na particular weight wk and input xi\\n∂LCE(ˆy,y)\\n∂wk,i\\n= −(yk −ˆyk)xi\\n= −(yk −p(yk = 1|x))xi\\n= −\\n(\\nyk − exp(wk ·x+bk)∑K\\nj=1 exp(wj ·x+bj)\\n)\\nxi (6.30)\\nBut these derivatives only give correct updates for one weight layer: the last one!\\nFor deep networks, computing the gradients for each weight is much more complex,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='j=1 exp(wj ·x+bj)\\n)\\nxi (6.30)\\nBut these derivatives only give correct updates for one weight layer: the last one!\\nFor deep networks, computing the gradients for each weight is much more complex,\\nsince we are computing the derivative with respect to weight parameters that appear\\nall the way back in the very early layers of the network, even though the loss is\\ncomputed only at the very end of the network.\\nThe solution to computing this gradient is an algorithm called error backprop-\\nagation or backprop (Rumelhart et al., 1986). While backprop was invented spe-error back-\\npropagation\\ncially for neural networks, it turns out to be the same as a more general procedure\\ncalled backward differentiation , which depends on the notion of computation\\ngraphs. Let’s see how that works in the next subsection.\\n6.6.3 Computation Graphs\\nA computation graph is a representation of the process of computing a mathematical\\nexpression, in which the computation is broken down into separate operations, each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6.3 Computation Graphs\\nA computation graph is a representation of the process of computing a mathematical\\nexpression, in which the computation is broken down into separate operations, each\\nof which is modeled as a node in a graph.\\nConsider computing the function L(a,b,c) =c(a +2b). If we make each of the\\ncomponent addition and multiplication operations explicit, and add names (d and e)\\nfor the intermediate outputs, the resulting series of computations is:\\nd = 2 ∗b\\ne = a +d\\nL = c ∗e\\nWe can now represent this as a graph, with nodes for each operation, and di-\\nrected edges showing the outputs from each operation as the inputs to the next, as\\nin Fig. 6.15. The simplest use of computation graphs is to compute the value of\\nthe function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,\\nb = 1, c = −2, and we’ve shown the result of the forward pass to compute the re-\\nsult L(3,1,−2) =−10. In the forward pass of a computation graph, we apply each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 21\\noperation left to right, passing the outputs of each computation as the input to the\\nnext node.\\ne=a+d\\nd = 2b L=ce\\na=3\\nb=1\\nc=-2\\ne=5d=2\\nL=-10\\nforward pass\\na\\nb\\nc\\nFigure 6.15 Computation graph for the functionL(a,b,c) =c(a+2b), with values for input\\nnodes a = 3, b = 1, c = −2, showing the forward pass computation of L.\\n6.6.4 Backward differentiation on computation graphs\\nThe importance of the computation graph comes from the backward pass, which\\nis used to compute the derivatives that we’ll need for the weight update. In this\\nexample our goal is to compute the derivative of the output function L with respect\\nto each of the input variables, i.e., ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c . The derivative ∂L\\n∂a tells us how\\nmuch a small change in a affects L.\\nBackwards differentiation makes use of the chain rule in calculus, so let’s re-chain rule\\nmind ourselves of that. Suppose we are computing the derivative of a composite'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Backwards differentiation makes use of the chain rule in calculus, so let’s re-chain rule\\nmind ourselves of that. Suppose we are computing the derivative of a composite\\nfunction f (x) =u(v(x)). The derivative of f (x) is the derivative ofu(x) with respect\\nto v(x) times the derivative of v(x) with respect to x:\\nd f\\ndx = du\\ndv ·dv\\ndx (6.31)\\nThe chain rule extends to more than two functions. If computing the derivative of a\\ncomposite function f (x) =u(v(w(x))), the derivative of f (x) is:\\nd f\\ndx = du\\ndv ·dv\\ndw ·dw\\ndx (6.32)\\nThe intuition of backward differentiation is to pass gradients back from the ﬁnal\\nnode to all the nodes in the graph. Fig. 6.16 shows part of the backward computation\\nat one node e. Each node takes an upstream gradient that is passed in from its parent\\nnode to the right, and for each of its inputs computes a local gradient (the gradient\\nof its output with respect to its input), and uses the chain rule to multiply these two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='node to the right, and for each of its inputs computes a local gradient (the gradient\\nof its output with respect to its input), and uses the chain rule to multiply these two\\nto compute a downstream gradient to be passed on to the next earlier node.\\nLet’s now compute the 3 derivatives we need. Since in the computation graph\\nL = ce, we can directly compute the derivative ∂L\\n∂c :\\n∂L\\n∂c = e (6.33)\\nFor the other two, we’ll need to use the chain rule:\\n∂L\\n∂a = ∂L\\n∂e\\n∂e\\n∂a\\n∂L\\n∂b = ∂L\\n∂e\\n∂e\\n∂d\\n∂d\\n∂b (6.34)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='22 CHAPTER 6 • N EURAL NETWORKS\\ned L\\ned\\n∂L\\n∂d\\n∂L\\n∂e= ∂e\\n∂d\\n∂L\\n∂e\\n∂e\\n∂d\\nupstream\\n gradientdownstream\\n gradient\\nlocal\\n gradient\\nFigure 6.16 Each node (like e here) takes an upstream gradient, multiplies it by the local\\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\\na downstream gradient to be passed on to a prior node. A node may have multiple local\\ngradients if it has multiple inputs.\\nEq. 6.34 and Eq. 6.33 thus require ﬁve intermediate derivatives: ∂L\\n∂e , ∂L\\n∂c , ∂e\\n∂a , ∂e\\n∂d , and\\n∂d\\n∂b , which are as follows (making use of the fact that the derivative of a sum is the\\nsum of the derivatives):\\nL = ce : ∂L\\n∂e = c, ∂L\\n∂c = e\\ne = a +d : ∂e\\n∂a = 1, ∂e\\n∂d = 1\\nd = 2b : ∂d\\n∂b = 2\\nIn the backward pass, we compute each of these partials along each edge of the\\ngraph from right to left, using the chain rule just as we did above. Thus we begin by\\ncomputing the downstream gradients from nodeL, which are ∂L\\n∂e and ∂L\\n∂c . For node e,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='graph from right to left, using the chain rule just as we did above. Thus we begin by\\ncomputing the downstream gradients from nodeL, which are ∂L\\n∂e and ∂L\\n∂c . For node e,\\nwe then multiply this upstream gradient ∂L\\n∂e by the local gradient (the gradient of the\\noutput with respect to the input), ∂e\\n∂d to get the output we send back to node d: ∂L\\n∂d .\\nAnd so on, until we have annotated the graph all the way to all the input variables.\\nThe forward pass conveniently already will have computed the values of the forward\\nintermediate variables we need (liked and e) to compute these derivatives. Fig. 6.17\\nshows the backward pass.\\ne=d+a\\nd = 2b L=ce\\na=3\\nb=1\\ne=5d=2\\nL=-10\\n \\na\\nb\\nc ∂L=5∂c\\n∂L =-2∂e\\n∂e =1∂d\\n∂d =2∂b\\n∂e =1∂a\\nbackward pass\\nc=-2\\n∂L =-2∂e\\n∂L =5∂c\\n∂L\\n∂d =-2∂e\\n∂d\\n∂L\\n∂e=\\n∂L\\n∂a =-2∂e\\n∂a\\n∂L\\n∂e=\\n∂L\\n∂b =-4∂d\\n∂b\\n∂L\\n∂d=\\nFigure 6.17 Computation graph for the function L(a,b,c) =c(a +2b), showing the backward pass computa-\\ntion of ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 23\\nBackward differentiation for a neural network\\nOf course computation graphs for real neural networks are much more complex.\\nFig. 6.18 shows a sample computation graph for a 2-layer neural network with n0 =\\n2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid\\noutput unit for simplicity. The function that the computation graph is computing is:\\nz[1] = W[1]x+b[1]\\na[1] = ReLU(z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = σ(z[2])\\nˆy = a[2] (6.35)\\nFor the backward pass we’ll also need to compute the loss L. The loss function\\nfor binary sigmoid output from Eq. 6.25 is\\nLCE (ˆy,y) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.36)\\nOur output ˆy = a[2], so we can rephrase this as\\nLCE (a[2],y) = −\\n[\\nyloga[2] +(1 −y)log(1 −a[2])\\n]\\n(6.37)\\nz[2] = \\n+ a[2] = σ\\n \\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1] *\\n*\\n*\\n*\\nx1\\nx2\\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\nw[2]\\n11\\nw[1]\\n11\\nw[1]\\n12\\nw[1]\\n21\\nw[1]\\n22 b[2]\\nw[2]\\n12\\nL (a[2],y)1\\n2\\n1\\n1 1\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content=']\\n(6.37)\\nz[2] = \\n+ a[2] = σ\\n \\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1] *\\n*\\n*\\n*\\nx1\\nx2\\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\nw[2]\\n11\\nw[1]\\n11\\nw[1]\\n12\\nw[1]\\n21\\nw[1]\\n22 b[2]\\nw[2]\\n12\\nL (a[2],y)1\\n2\\n1\\n1 1\\n22\\nFigure 6.18 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\\nand 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]\\n11 means\\nthat w[1]\\n11 is to be multiplied by x1, and the node z[1] = +means that the value of z[1] is computed by summing\\nthe three nodes that feed into it (the two products, and the bias term b[1]\\ni ).\\nThe weights that need updating (those for which we need to know the partial\\nderivative of the loss function) are shown in teal. In order to do the backward pass,\\nwe’ll need to know the derivatives of all the functions in the graph. We already saw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='derivative of the loss function) are shown in teal. In order to do the backward pass,\\nwe’ll need to know the derivatives of all the functions in the graph. We already saw\\nin Section ?? the derivative of the sigmoid σ:\\ndσ(z)\\ndz = σ(z)(1 −σ(z)) (6.38)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='24 CHAPTER 6 • N EURAL NETWORKS\\nWe’ll also need the derivatives of each of the other activation functions. The\\nderivative of tanh is:\\nd tanh(z)\\ndz = 1 −tanh2(z) (6.39)\\nThe derivative of the ReLU is2\\nd ReLU(z)\\ndz =\\n{0 f or z < 0\\n1 f or z ≥0 (6.40)\\nWe’ll give the start of the computation, computing the derivative of the loss function\\nL with respect to z, or ∂L\\n∂z (and leaving the rest of the computation as an exercise for\\nthe reader). By the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z (6.41)\\nSo let’s ﬁrst compute ∂L\\n∂a[2] , taking the derivative of Eq. 6.37, repeated here:\\nLCE (a[2],y) = −\\n[\\nyloga[2] +(1 −y)log(1 −a[2])\\n]\\n∂L\\n∂a[2] = −\\n((\\ny∂ log(a[2])\\n∂a[2]\\n)\\n+(1 −y)∂ log(1 −a[2])\\n∂a[2]\\n)\\n= −\\n((\\ny 1\\na[2]\\n)\\n+(1 −y) 1\\n1 −a[2] (−1)\\n)\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\n(6.42)\\nNext, by the derivative of the sigmoid:\\n∂a[2]\\n∂z = a[2](1 −a[2])\\nFinally, we can use the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\na[2](1 −a[2])\\n= a[2] −y (6.43)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content=')\\n(6.42)\\nNext, by the derivative of the sigmoid:\\n∂a[2]\\n∂z = a[2](1 −a[2])\\nFinally, we can use the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\na[2](1 −a[2])\\n= a[2] −y (6.43)\\nContinuing the backward computation of the gradients (next by passing the gra-\\ndients over b[2]\\n1 and the two product nodes, and so on, back to all the teal nodes), is\\nleft as an exercise for the reader.\\n6.6.5 More details on learning\\nOptimization in neural networks is a non-convex optimization problem, more com-\\nplex than for logistic regression, and for that and other reasons there are many best\\npractices for successful learning.\\n2 The derivative is actually undeﬁned at the point z = 0, but by convention we treat it as 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.7 • S UMMARY 25\\nFor logistic regression we can initialize gradient descent with all the weights and\\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\\nweights with small random numbers. It’s also helpful to normalize the input values\\nto have 0 mean and unit variance.\\nVarious forms of regularization are used to prevent overﬁtting. One of the most\\nimportant is dropout: randomly dropping some units and their connections fromdropout\\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\\neach unit we replace its output with zero with probability p (and renormalize the\\nrest of the outputs from that layer).\\nTuning of hyperparameters is also important. The parameters of a neural net-hyperparameter'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='rest of the outputs from that layer).\\nTuning of hyperparameters is also important. The parameters of a neural net-hyperparameter\\nwork are the weights W and biases b; those are learned by gradient descent. The\\nhyperparameters are things that are chosen by the algorithm designer; optimal val-\\nues are tuned on a devset rather than by gradient descent learning on the training\\nset. Hyperparameters include the learning rate η, the mini-batch size, the model\\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\\nof activation functions), how to regularize, and so on. Gradient descent itself also\\nhas many architectural variants such as Adam (Kingma and Ba, 2015).\\nFinally, most modern neural networks are built using computation graph for-\\nmalisms that make it easy and natural to do gradient computation and parallelization\\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='malisms that make it easy and natural to do gradient computation and parallelization\\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\\nreader should consult a neural network textbook for further details; some sugges-\\ntions are at the end of the chapter.\\n6.7 Summary\\n• Neural networks are built out ofneural units, originally inspired by biological\\nneurons but now simply an abstract computational device.\\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\\nthen applies a non-linear activation function like sigmoid, tanh, or rectiﬁed\\nlinear unit.\\n• In a fully-connected, feedforward network, each unit in layer i is connected\\nto each unit in layer i +1, and there are no cycles.\\n• The power of neural networks comes from the ability of early layers to learn\\nrepresentations that can be utilized by later layers in the network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='• The power of neural networks comes from the ability of early layers to learn\\nrepresentations that can be utilized by later layers in the network.\\n• Neural networks are trained by optimization algorithms like gradient de-\\nscent.\\n• Error backpropagation, backward differentiation on a computation graph,\\nis used to compute the gradients of the loss function for a network.\\n• Neural language models use a neural network as a probabilistic classiﬁer, to\\ncompute the probability of the next word given the previous n words.\\n• Neural language models can use pretrained embeddings, or can learn embed-\\ndings from scratch in the process of language modeling.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='26 CHAPTER 6 • N EURAL NETWORKS\\nHistorical Notes\\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\\nloch and Pitts, 1943), a simpliﬁed model of the biological neuron as a kind of com-\\nputing element that could be described in terms of propositional logic. By the late\\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\\nBernard Widrow at Stanford) developed research into neural networks; this phase\\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\\nThe ﬁeld of neural networks declined after it was shown that a single perceptron\\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\\nWhile some small amount of work continued during the next two decades, a major\\nrevival for the ﬁeld didn’t come until the 1980s, when practical tools for building'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='While some small amount of work continued during the next two decades, a major\\nrevival for the ﬁeld didn’t come until the 1980s, when practical tools for building\\ndeeper networks like error backpropagation became widespread (Rumelhart et al.,\\n1986). During the 1980s a wide variety of neural network and related architec-\\ntures were developed, particularly for applications in psychology and cognitive sci-\\nence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\\nand McClelland 1986a, Elman 1990), for which the term connectionist or paral-connectionist\\nlel distributed processing was often used (Feldman and Ballard 1982, Smolensky\\n1988). Many of the principles and techniques developed in this period are foun-\\ndational to modern work, including the ideas of distributed representations (Hinton,\\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\\n(Smolensky, 1990).\\nBy the 1990s larger neural networks began to be applied to many practical lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\\n(Smolensky, 1990).\\nBy the 1990s larger neural networks began to be applied to many practical lan-\\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\\nspeech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\\nin computer hardware and advances in optimization and training techniques made it\\npossible to train even larger and deeper networks, leading to the modern term deep\\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\\nChapter 13 and Chapter 15.\\nThere are a number of excellent books on neural networks, including Goodfellow\\net al. (2016) and Nielsen (2015).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Historical Notes 27\\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-\\nard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-\\nberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,\\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-\\nwar, P. Tucker, V . Vanhoucke, V . Vasudevan, F. Vi´egas,\\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y . Yu,\\nand X. Zheng. 2015. TensorFlow: Large-scale machine\\nlearning on heterogeneous systems. Software available\\nfrom tensorﬂow.org.\\nBengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\nA neural probabilistic language model. JMLR, 3:1137–\\n1155.\\nBengio, Y ., P. Lamblin, D. Popovici, and H. Larochelle.\\n2007. Greedy layer-wise training of deep networks.\\nNeurIPS.\\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\\nence, 14(2):179–211.\\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='NeurIPS.\\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\\nence, 14(2):179–211.\\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-\\nels and their properties. Cognitive Science, 6:205–254.\\nGoodfellow, I., Y . Bengio, and A. Courville. 2016. Deep\\nLearning. MIT Press.\\nHinton, G. E. 1986. Learning distributed representations of\\nconcepts. COGSCI.\\nHinton, G. E., S. Osindero, and Y .-W. Teh. 2006. A fast\\nlearning algorithm for deep belief nets. Neural computa-\\ntion, 18(7):1527–1554.\\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. 2012. Improving neural networks\\nby preventing co-adaptation of feature detectors. ArXiv\\npreprint arXiv:1207.0580.\\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\\noptimization. ICLR 2015.\\nLeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\\nagation applied to handwritten zip code recognition.Neu-\\nral computation, 1(4):541–551.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='LeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\\nagation applied to handwritten zip code recognition.Neu-\\nral computation, 1(4):541–551.\\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\\nof speech perception. Cognitive Psychology, 18:1–86.\\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\\nideas immanent in nervous activity. Bulletin of Mathe-\\nmatical Biophysics, 5:115–133.\\nMinsky, M. and S. Papert. 1969. Perceptrons. MIT Press.\\nMorgan, N. and H. Bourlard. 1990. Continuous speech\\nrecognition using multilayer perceptrons with hidden\\nmarkov models. ICASSP.\\nNielsen, M. A. 2015. Neural networks and Deep learning .\\nDetermination Press USA.\\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n2017. Automatic differentiation in pytorch. NIPS-W.\\nRosenblatt, F. 1958. The perceptron: A probabilistic model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n2017. Automatic differentiation in pytorch. NIPS-W.\\nRosenblatt, F. 1958. The perceptron: A probabilistic model\\nfor information storage and organization in the brain.Psy-\\nchological review, 65(6):386–408.\\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\\nLearning internal representations by error propagation. In\\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\\ntributed Processing, volume 2, 318–362. MIT Press.\\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\\nthe past tense of English verbs. In D. E. Rumelhart and\\nJ. L. McClelland, eds, Parallel Distributed Processing,\\nvolume 2, 216–271. MIT Press.\\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\\nDistributed Processing. MIT Press.\\nRussell, S. and P. Norvig. 2002. Artiﬁcial Intelligence: A\\nModern Approach, 2nd edition. Prentice Hall.\\nSmolensky, P. 1988. On the proper treatment of connection-\\nism. Behavioral and brain sciences, 11(1):1–23.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Modern Approach, 2nd edition. Prentice Hall.\\nSmolensky, P. 1988. On the proper treatment of connection-\\nism. Behavioral and brain sciences, 11(1):1–23.\\nSmolensky, P. 1990. Tensor product variable binding and\\nthe representation of symbolic structures in connectionist\\nsystems. Artiﬁcial intelligence, 46(1-2):159–216.\\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\\nand R. R. Salakhutdinov. 2014. Dropout: a simple\\nway to prevent neural networks from overﬁtting. JMLR,\\n15(1):1929–1958.\\nWidrow, B. and M. E. Hoff. 1960. Adaptive switching cir-\\ncuits. IRE WESCON Convention Record, volume 4.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49eac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 356 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:03<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (356, 384)\n",
      "Adding 356 documents to vector store...\n",
      "Successfully added 356 documents to vector store\n",
      "Total documents in collection: 356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3fabc",
   "metadata": {},
   "source": [
    "Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32690228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f8abd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x11eb25010>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30fdb28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_200c8466_316',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 2,\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'title': '',\n",
       "   'source': '../data/pdf/attention.pdf',\n",
       "   'doc_index': 316,\n",
       "   'trapped': '/False',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'content_length': 216,\n",
       "   'page_label': '3',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'},\n",
       "  'similarity_score': 0.13995516300201416,\n",
       "  'distance': 0.8600448369979858,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882c61b",
   "metadata": {},
   "source": [
    "RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
